{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "src_path = Path('.').absolute().parent\n",
    "data_path = src_path / 'data'\n",
    "sys.path.append(str(src_path / 'src'))\n",
    "\n",
    "import yaml\n",
    "import networkx as nx\n",
    "from ontology import OntologySystem\n",
    "\n",
    "with (src_path / 'setting_files' / 'app_settings.yml').open('r') as file:\n",
    "    settings = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "onto = OntologySystem(\n",
    "    acc_name_path=data_path / 'AccountName.csv', \n",
    "    rdf_path=data_path / 'AccountRDF.xml',\n",
    "    model_path=data_path / settings['ontology']['model']['model_name'],\n",
    "    kwargs_graph_drawer=settings['ontology']['graph_drawer']\n",
    ")\n",
    "ACC_DICT = onto.ACC_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: position 만들기\n",
    "# (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]})\n",
    "\n",
    "def get_entity(s, x, tag):\n",
    "    idx = s.index(x)\n",
    "    return (idx, idx+len(x), tag)\n",
    "\n",
    "def random_sampling(x_dict, x_key):\n",
    "    idx_range = np.arange(len(x_dict[x_key]))\n",
    "    idx = np.random.choice(idx_range, replace=False, p=np.ones(len(idx_range)) / len(idx_range))\n",
    "    word, tag, desc = x_dict[x_key][idx]\n",
    "    return word, tag, desc\n",
    "\n",
    "def get_words_filtered(words, text):\n",
    "    words_filtered = defaultdict(list)\n",
    "    for k, v in words.items():\n",
    "        for word, tag, desc in v:\n",
    "            if desc != text:\n",
    "                words_filtered[k].append((word, tag, desc))\n",
    "    return words_filtered\n",
    "\n",
    "df = pd.read_csv(data_path / 'AccountWords.csv', encoding='utf-8')\n",
    "\n",
    "format_dict = {\n",
    "    0: ['help'],\n",
    "    1: [\n",
    "        # what/how, target_account, [MASK] + year/quarter\n",
    "        \"{} is the {} in the {} ?\",\n",
    "        # [MASK] + year/quarter, what/how, target_account\n",
    "        \"In the {}, {} is the value of the {} ?\"\n",
    "    ], \n",
    "    2: [\n",
    "        # target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "        \"what happens to the {} when the {} {} by {} in the {} ?\",\n",
    "        # target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "        \"what will be the effect to {} if the {} {} by {} in the {} ?\",\n",
    "        # reverse the relation\n",
    "        # subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "        \"when the {} {} by {} in the {}, what will happen to the {} ?\",\n",
    "        # subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "        \"if the {} {} by {} in the {}, what will be the effect to {} ?\"\n",
    "    ],\n",
    "    3: [\n",
    "        # what/how, target_account, [MASK] + year/quarter\n",
    "        \"{} will be the {} in the {} ?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# TODO: maybe add the today's information after [SEP]?\n",
    "context = ['HELP', 'PAST', 'FUTURE']\n",
    "words = defaultdict(list)\n",
    "for typ in ['year', 'quarter', 'words']:\n",
    "    df_temp = df.loc[:, [typ, f'{typ}_tag', f'{typ}_desc']]\n",
    "    df_temp = df_temp.loc[~df_temp[typ].isna(), :]\n",
    "    for i, (w, t, desc) in df_temp.iterrows():\n",
    "        words[typ].append((w, t, desc))\n",
    "\n",
    "exceptions = ['BalanceSheet', 'IncomeStatement', 'Ratios', 'CalendarOneYear']\n",
    "times = ['year', 'quarter']\n",
    "\n",
    "all_data = []\n",
    "s_ENT = '[E]'\n",
    "e_ENT = '[/E]'\n",
    "f_ENT = lambda x: f'{s_ENT}{x}{e_ENT}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "```python\n",
    "# what/how, target_account, [MASK] + year/quarter\n",
    "\"{} is the {} in the {}?\",\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062d15cb84954c2fb79e64052efda37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data1 = []\n",
    "trg_scenario = 1\n",
    "progress_bar = tqdm()\n",
    "words_filtered = get_words_filtered(words, text='FUTURE')\n",
    "for idx_fmt, fmt in enumerate(format_dict[trg_scenario]):\n",
    "    \n",
    "    for acc, dic in ACC_DICT.items():\n",
    "        if acc in exceptions:\n",
    "            continue\n",
    "        target_account = dic['eng_name'].lower()\n",
    "        knowledge, acc_type, _ = dic['group'].split('-')\n",
    "\n",
    "        for t in ['year', 'quarter']:\n",
    "            for t_word, t_tag, _ in words_filtered[t]:\n",
    "                entities = []\n",
    "                pre_token = np.random.choice(['what', 'how'], replace=False, p=np.ones(2)/2)\n",
    "                if idx_fmt == 0:\n",
    "                    # what/how, target_account, [MASK] + year/quarter\n",
    "                    # \"{} is the {} in the {}?\",\n",
    "                    s = fmt.format(\n",
    "                        pre_token,\n",
    "                        f_ENT(target_account), \n",
    "                        f_ENT(f'{t_word} {t}')\n",
    "                        )\n",
    "                else:\n",
    "                    # [MASK] + year/quarter, what/how, target_account\n",
    "                    # \"In the {}, {} is the value of the {}\"\n",
    "                    s = fmt.format(\n",
    "                        f_ENT(f'{t_word} {t}'),\n",
    "                        pre_token,\n",
    "                        f_ENT(target_account)\n",
    "                    )\n",
    "                # relation = [0, 0, 0]  # no_relation, order1, order2\n",
    "                # entities\n",
    "                ## target_account\n",
    "                entities.append(get_entity(s, f_ENT(target_account), f'{knowledge}.{acc_type}'))\n",
    "                ## MASK year/quarter\n",
    "                entities.append(get_entity(s, f_ENT(f'{t_word} {t}'), t_tag))\n",
    "                \n",
    "                data1.append(\n",
    "                    {'question': s, 'entities': sorted(entities, key=lambda x: x[0]), 'intent': 'PAST.value'} #, 'relation': relation}\n",
    "                )\n",
    "            \n",
    "                progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "```python\n",
    "# target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "\"what happens to the {} when the {} {} by {} in the {}?\"\n",
    "# target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "\"what will be the effect to {} if the {} {} by {} in the {}?\"\n",
    "# reverse the relation\n",
    "# subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "\"when the {} {} by {} in the {}, what will happen to the {}?\"\n",
    "# subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "\"if the {} {} by {} in the {}, what will be the effect to {}?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_role_dict(onto, knowledge):\n",
    "    knowledge_query = onto.sparql.get_predefined_knowledge(knowledge=knowledge)\n",
    "    sparql_results = onto.sparql.query(knowledge_query)\n",
    "    role_dict = defaultdict(list)\n",
    "    for s, p, o in sparql_results:\n",
    "        s, p, o = map(onto.graph_drawer.convert_to_string, [s, p, o])\n",
    "        if s == 'CalendarOneYear' or o == 'CalendarOneYear':\n",
    "            continue\n",
    "        if s not in role_dict[o]:\n",
    "            role_dict[o].append(s)\n",
    "        \n",
    "    return role_dict\n",
    "\n",
    "def process_successor(successors, role_dict, trg_acc, acc):\n",
    "    if role_dict.get(acc) is None:\n",
    "        # successors[trg_acc].extend(successors[acc])\n",
    "        return None\n",
    "    else:\n",
    "        accs = role_dict.get(acc)\n",
    "        if accs is not None:\n",
    "            successors[trg_acc].extend(accs)\n",
    "            for acc in accs:\n",
    "                process_successor(successors, role_dict, trg_acc, acc)\n",
    "\n",
    "def get_successor(onto, knowledge, exceptions=None):\n",
    "    role_dict = get_role_dict(onto, knowledge=knowledge)\n",
    "    successors = defaultdict(list)\n",
    "    for trg_acc in role_dict.keys():\n",
    "        if (exceptions is not None) and (trg_acc in exceptions):\n",
    "            continue\n",
    "        process_successor(successors, role_dict, trg_acc, trg_acc)\n",
    "    return successors\n",
    "\n",
    "trg_scenario = 2\n",
    "bs_successors = get_successor(sparql, 'BS', exceptions)\n",
    "is_successors = get_successor(sparql, 'IS', exceptions)\n",
    "data2 = []\n",
    "n_sample = 5\n",
    "progress_bar = tqdm()\n",
    "words_filtered = get_words_filtered(words, text='FUTURE')\n",
    "\n",
    "for idx_fmt, fmt in enumerate(format_dict[trg_scenario]):\n",
    "    for sub_tree in [bs_successors, is_successors]:\n",
    "        for trg_acc, successors in sub_tree.items():\n",
    "            if trg_acc in exceptions:\n",
    "                continue\n",
    "            target_account = ACC_DICT[trg_acc]['eng_name'].lower()\n",
    "            target_knowledge, target_acc_type, _ = ACC_DICT[trg_acc]['group'].split('-')\n",
    "            for sub_acc in successors:\n",
    "                subject_account = ACC_DICT[sub_acc]['eng_name'].lower()\n",
    "                subject_knowledge, subject_acc_type, _ = ACC_DICT[trg_acc]['group'].split('-')\n",
    "                n = 0\n",
    "                while n < n_sample:\n",
    "                    entities = []\n",
    "\n",
    "                    apply_word, apply_tag, _ = random_sampling(x_dict=words_filtered, x_key='words')\n",
    "                    t = np.random.choice(times, replace=False, p=np.ones(len(times))/len(times))\n",
    "                    t_word, t_tag, _ = random_sampling(x_dict=words_filtered, x_key=t)\n",
    "                    \n",
    "                    number = np.random.randint(1, 99)\n",
    "                    percent = np.random.choice(['percent', '%'], replace=False, p=np.ones(2)/2)\n",
    "                    \n",
    "                    if idx_fmt in [0, 1]:\n",
    "                        # target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "                        s = fmt.format(\n",
    "                            f_ENT(target_account),\n",
    "                            f_ENT(subject_account), \n",
    "                            f_ENT(apply_word), \n",
    "                            f_ENT(f'{number} {percent}'),\n",
    "                            f_ENT(f'{t_word} {t}')\n",
    "                            )\n",
    "                        # relation = [1, 1, 2]\n",
    "                    else:\n",
    "                        # subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "                        s = fmt.format(\n",
    "                            f_ENT(subject_account), \n",
    "                            f_ENT(apply_word), \n",
    "                            f_ENT(f'{number} {percent}'),\n",
    "                            f_ENT(f'{t_word} {t}'),\n",
    "                            f_ENT(target_account)\n",
    "                            )\n",
    "                        # relation = [1, 2, 1]\n",
    "                    # entities\n",
    "                    ## target_account\n",
    "                    entities.append(get_entity(s, f_ENT(target_account), f'{target_knowledge}.{target_acc_type}'))\n",
    "                    ## subject_account\n",
    "                    entities.append(get_entity(s, f_ENT(subject_account), f'{subject_knowledge}.{subject_acc_type}'))\n",
    "                    ## MASK apply words\n",
    "                    entities.append(get_entity(s, f_ENT(apply_word), apply_tag))\n",
    "                    ## percentages\n",
    "                    entities.append(get_entity(s, f_ENT(f'{number} {percent}'), 'PERCENT'))\n",
    "                    ## MASK year/quarter\n",
    "                    entities.append(get_entity(s, f_ENT(f'{t_word} {t}'), t_tag))\n",
    "\n",
    "                    d = {'question': s, 'entities': sorted(entities, key=lambda x: x[0]), 'intent': 'IF.fact'} #, 'relation': relation}\n",
    "                    if d not in data2:\n",
    "                        data2.append(\n",
    "                            d\n",
    "                        )\n",
    "                    \n",
    "                    progress_bar.update(1)\n",
    "                    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "```python\n",
    "# what/how, target_account, [MASK] + year/quarter\n",
    "\"{} will be the {} in the {}?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119fd2afaac74f688b65fd2e1c464fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n"
     ]
    }
   ],
   "source": [
    "data3 = []\n",
    "trg_scenario = 3\n",
    "progress_bar = tqdm()\n",
    "words_filtered = get_words_filtered(words, text='PAST')\n",
    "\n",
    "for fmt in format_dict[trg_scenario]:\n",
    "    for acc, dic in ACC_DICT.items():\n",
    "        if acc in exceptions:\n",
    "            continue\n",
    "        target_account = dic['eng_name'].lower()\n",
    "        knowledge, acc_type, _ = dic['group'].split('-')\n",
    "        for t in ['year', 'quarter']:\n",
    "            for t_word, t_tag, _ in words_filtered[t]:\n",
    "                entities = []\n",
    "                s = fmt.format(\n",
    "                    np.random.choice(['what', 'how']), \n",
    "                    f_ENT(target_account), \n",
    "                    f_ENT(f'{t_word} {t}')\n",
    "                    )\n",
    "                # relation = [0, 0, 0]\n",
    "                # entities\n",
    "                ## target_account\n",
    "                entities.append(get_entity(s, f_ENT(target_account), f'{knowledge}.{acc_type}'))\n",
    "                ## MASK year/quarter\n",
    "                entities.append(get_entity(s, f_ENT(f'{t_word} {t}'), t_tag))\n",
    "                \n",
    "                data3.append(\n",
    "                    {'question': s, 'entities': entities, 'intent': 'IF.forecast'} #, 'relation': relation}\n",
    "                )\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "\n",
    "all_data = data1 + data2 + data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "special_len = len(s_ENT)+len(e_ENT)\n",
    "\n",
    "for k, x in tqdm(enumerate(all_data), total=len(all_data)):\n",
    "    all_data[k]['question'] = x['question'].replace(s_ENT, '').replace(e_ENT, '')\n",
    "    for i, (s, e, ent) in enumerate(x['entities']):\n",
    "        new_s = s-i*special_len\n",
    "        new_e = new_s+(e-s)-special_len\n",
    "        all_data[k]['entities'][i] = (new_s, new_e, ent)\n",
    "\n",
    "with (data_path / 'all_data.jsonl').open('w', encoding='utf-8') as file:\n",
    "    for line in tqdm(all_data, total=len(all_data), desc='saving'):\n",
    "        file.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning\n",
    "\n",
    "- Entities\n",
    "- Entities Relation (subject, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12188/1278096664.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlu_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNLUDataModule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNLUTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNLUDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmain_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain_path\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'data'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msetting_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain_path\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'setting_files'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.nlu_utils import NLUDataModule, NLUTokenizer, NLUDataset\n",
    "\n",
    "main_path = Path().absolute().parent\n",
    "data_path = main_path / 'data'\n",
    "setting_path = main_path / 'setting_files'\n",
    "\n",
    "with (setting_path / 'train_settings.yml').open('r') as file:\n",
    "    settings = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "nlu_tokenizer = NLUTokenizer()\n",
    "\n",
    "data_module = NLUDataModule(\n",
    "    train_path=data_path / settings['train_file'], \n",
    "    valid_path=data_path / settings['valid_file'],\n",
    "    test_path=data_path / settings['test_file'],\n",
    "    labels_path=data_path / settings['labels_file'],\n",
    "    batch_size=settings['batch_size'], \n",
    "    max_len=settings['max_len'],\n",
    "    num_workers=settings['num_workers'],\n",
    "    seed=settings['seed']\n",
    ")\n",
    "data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_module.train_data\n",
    "train_dataset = NLUDataset(\n",
    "    train_data, \n",
    "    tags2id=data_module.tags2id, \n",
    "    intents2id=data_module.intents2id,\n",
    ")\n",
    "train_dataloader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  3,  0,  0, 13, 14,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['tags'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter()\n",
    "for l in x['tags']:\n",
    "    c.update(l.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (C:\\Users\\simon\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n",
      "100%|██████████| 3/3 [00:00<00:00, 430.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "conll = load_dataset('conll2003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = conll['train']\n",
    "feature = dataset.features['ner_tags'].feature\n",
    "errors = 0\n",
    "x = dataset[1343]\n",
    "text = ' '.join(x['tokens']).lower()\n",
    "doc = nlu_tokenizer.spacy_nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'west', 'bank', 'bookseller', 'charged', 'on', 'thursday', 'that', 'the', 'palestinian', 'information', 'ministry', 'has', 'forced', 'him', 'to', 'sign', 'an', 'undertaking', 'not', 'to', 'distribute', 'books', 'written', 'by', 'critics', 'of', 'israeli-plo', 'self-rule', 'deals', '.']\n",
      "['O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O']\n",
      "['a', 'west', 'bank', 'bookseller', 'charged', 'on', 'thursday', 'that', 'the', 'palestinian', 'information', 'ministry', 'has', 'forced', 'him', 'to', 'sign', 'an', 'undertaking', 'not', 'to', 'distribute', 'books', 'written', 'by', 'critics', 'of', 'israeli', '-', 'plo', 'self', '-', 'rule', 'deals', '.']\n",
      "['a', 'west', 'bank', 'books', '##eller', 'charged', 'on', 'thursday', 'that', 'the', 'palestinian', 'information', 'ministry', 'has', 'forced', 'him', 'to', 'sign', 'an', 'undertaking', 'not', 'to', 'distribute', 'books', 'written', 'by', 'critics', 'of', 'israeli', '-', 'pl', '##o', 'self', '-', 'rule', 'deals', '.']\n",
      "[31, 31, 35, 37]\n"
     ]
    }
   ],
   "source": [
    "doc\n",
    "spacy_tokens = list(map(str, doc))\n",
    "tags = list(map(feature.int2str, x['ner_tags']))\n",
    "bert_tokens = nlu_tokenizer.bert_tokenize(text)\n",
    "original_tokens = list(map(str.lower, x['tokens']))\n",
    "\n",
    "print(original_tokens)\n",
    "print(tags)\n",
    "print(spacy_tokens)\n",
    "print(bert_tokens)\n",
    "print(list(map(len, [original_tokens, tags, spacy_tokens, bert_tokens])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizations import get_alignments\n",
    "from spacy.training import biluo_tags_to_offsets, iob_to_biluo, biluo_to_iob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-LOC', 'L-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'L-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'L-MISC', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(mapped_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 11, 'LOC'), (52, 84, 'ORG'), (169, 180, 'MISC')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = biluo_tags_to_offsets(doc, mapped_tags)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ = 'train'\n",
    "dataset = conll[typ]\n",
    "feature = dataset.features['ner_tags'].feature\n",
    "errors = 0\n",
    "for x in tqdm(dataset, total=len(dataset), desc=f'{typ}set'):\n",
    "    text = ' '.join(x['tokens']).lower()\n",
    "    doc = nlu_tokenizer.spacy_nlp(text)\n",
    "\n",
    "    tags = list(map(feature.int2str, x['ner_tags']))\n",
    "    spacy_tokens = list(map(str, doc))\n",
    "    original_tokens = list(map(str.lower, x['tokens']))\n",
    "    mapped_tags = nlu_tokenizer.fix_tags_alignment(\n",
    "        longer_tokens=spacy_tokens, shorter_tokens=original_tokens, tags=tags\n",
    "    )\n",
    "\n",
    "    entities = biluo_tags_to_offsets(doc, mapped_tags)\n",
    "    if not entities:\n",
    "        errors += 1\n",
    "        continue\n",
    "\n",
    "    d = {'text': text, 'entities': entities, 'intent': 'None'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_tokens)\n",
    "print(tags)\n",
    "print(spacy_tokens)\n",
    "print(mapped_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2b, b2a = get_alignments(a=original_tokens, b=spacy_tokens)\n",
    "\n",
    "biluo_tags = iob_to_biluo(tags)\n",
    "mapped_tags = ['-'] * len(spacy_tokens)\n",
    "for i, tag in enumerate(biluo_tags):\n",
    "    if tag == 'O':\n",
    "        for k in a2b[i]:\n",
    "            mapped_tags[k] = tag\n",
    "        continue\n",
    "\n",
    "    prefix, label = tag.split('-')\n",
    "    if prefix == 'B':\n",
    "        for j, k in enumerate(a2b[i]):\n",
    "            if j == 0:\n",
    "                mapped_tags[k] = tag\n",
    "            else:\n",
    "                mapped_tags[k] = f'I-{label}'\n",
    "    elif prefix == 'L':\n",
    "        for j, k in enumerate(a2b[i]):\n",
    "            if j == len(a2b[i])-1:\n",
    "                mapped_tags[k] = tag\n",
    "            else:\n",
    "                mapped_tags[k] = f'I-{label}'\n",
    "    elif prefix == 'U':\n",
    "        if len(a2b[i]) == 1:\n",
    "            k = a2b[i][0]\n",
    "            mapped_tags[k] = tag\n",
    "        elif len(a2b[i]) == 2:\n",
    "            b, l = a2b[i]\n",
    "            mapped_tags[b] = f'B-{label}'\n",
    "            mapped_tags[l] = f'L-{label}'\n",
    "        else:\n",
    "            for j, k in enumerate(a2b[i]):\n",
    "                if j == 0:\n",
    "                    mapped_tags[k] = f'B-{label}'\n",
    "                elif j == len(a2b[i])-1:\n",
    "                    mapped_tags[k] = f'L-{label}'\n",
    "                else:\n",
    "                    mapped_tags[k] = f'I-{label}'\n",
    "    else:\n",
    "        for j, k in enumerate(a2b[i]):\n",
    "            mapped_tags[k] = tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: augmentation 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from spacy.training import biluo_tags_to_offsets, iob_to_biluo, offsets_to_biluo_tags, biluo_to_iob\n",
    "from src.nlu_createData import SparqlHandler\n",
    "\n",
    "s_ENT = '[E]'\n",
    "e_ENT = '[/E]'\n",
    "f_ENT = lambda x: f'{s_ENT}{x}{e_ENT}'\n",
    "def get_entity(s, x, tag):\n",
    "    idx = s.index(x)\n",
    "    return (idx, idx+len(x), tag)\n",
    "\n",
    "sparql = SparqlHandler(data_path / 'AccountRDF.xml')\n",
    "df_account = pd.read_csv(data_path / 'AccountName.csv', encoding='utf-8')\n",
    "ACC_DICT = defaultdict(dict)\n",
    "for _, row in df_account.iterrows():\n",
    "    acc = row['acc']\n",
    "    eng = row['acc_name_eng']\n",
    "    kor = row['acc_name_kor']\n",
    "    group = row['group']\n",
    "    ACC_DICT[acc]['kor_name'] = kor\n",
    "    ACC_DICT[acc]['eng_name'] = eng\n",
    "    ACC_DICT[acc]['group'] = group\n",
    "\n",
    "df = pd.read_csv(data_path / 'AccountWords.csv', encoding='utf-8')\n",
    "\n",
    "format_dict = {\n",
    "    0: [\n",
    "        # only to train the ner task\n",
    "        # account \n",
    "    ],\n",
    "    1: [\n",
    "        # what/how, target_account, [MASK] + year/quarter\n",
    "        \"{} is the {} in the {}?\",\n",
    "    ], \n",
    "    2: [\n",
    "        # target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "        \"what happens to the {} when the {} {} by {} in the {}?\",\n",
    "        # target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "        \"what will be the effect to {} if the {} {} by {} in the {}?\",\n",
    "        # reverse the relation\n",
    "        # subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "        \"when the {} {} by {} in the {}, what will happen to the {}?\",\n",
    "        # subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "        \"if the {} {} by {} in the {}, what will be the effect to {}?\"\n",
    "    ],\n",
    "    3: [\n",
    "        # what/how, target_account, [MASK] + year/quarter\n",
    "        \"{} will be the {} in the {}?\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model_p = 'bert-base-uncased' # 'albert-base-v2' / 'bert-base-uncased'\n",
    "# fillmask = pipeline('fill-mask', model=model_p)\n",
    "# generator = pipeline('text-generation', model='gpt2')\n",
    "# mask_token = fillmask.tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator(f'{target_account}', max_length=50, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "# , BertTokenizerFast, BertForMaskedLM, AlbertForMaskedLM, AlbertTokenizerFast\n",
    "\n",
    "# model_p = 'bert-base-uncased' # 'albert-base-v2' / 'bert-base-uncased'\n",
    "# fillmask = pipeline('fill-mask', model=model_p)\n",
    "# generator = pipeline('text-generation', model='gpt2')\n",
    "# mask_token = fillmask.tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_multi_mask(bert_model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = bert_model(**inputs)\n",
    "    predictions = outputs[0]\n",
    "    return predictions\n",
    "\n",
    "def template_generator(mask_token, max_length, acc_idxes, target_account):\n",
    "    preset = [mask_token] * max_length\n",
    "    candidates = []\n",
    "    for i in acc_idxes:\n",
    "        temp = deepcopy(preset)\n",
    "        temp[i] = target_account\n",
    "        candidates.append(' '.join(temp))\n",
    "    return candidates\n",
    "\n",
    "def get_aug_sequences(tokenizer, model, cands, top_k, max_length):\n",
    "    number_of_mask_token = max_length-1\n",
    "    m_l = 0\n",
    "    while m_l < number_of_mask_token:\n",
    "        cands_tokens = [tokenizer.tokenize(c, padding=True, truncation=True) for c in cands]\n",
    "        infer_indices = []\n",
    "        for c in cands_tokens:\n",
    "            mask_idx = list(np.arange(len(c))[np.array(c) == '[MASK]'])\n",
    "            choosed_idx = np.random.choice(mask_idx)\n",
    "            infer_indices.append(choosed_idx)\n",
    "        \n",
    "        inputs = tokenizer(cands, padding=True, truncation=True, return_tensors='pt')\n",
    "        o = model(**inputs)[0][:, 1:-1]\n",
    "        _, sorted_idx = o.sort(dim=-1, descending=True)\n",
    "        predict_ids = sorted_idx[torch.arange(sorted_idx.size(0)), infer_indices, :top_k]\n",
    "\n",
    "        cands_tensors = inputs['input_ids'][:, 1:-1]\n",
    "        cands_tensors = cands_tensors.unsqueeze(1).expand(cands_tensors.size(0), top_k, cands_tensors.size(1)).clone()\n",
    "        cands_tensors[torch.arange(cands_tensors.size(0)), :, infer_indices] = predict_ids\n",
    "        cands = [tokenizer.decode(c) for c in cands_tensors.view(cands_tensors.size(0)*top_k, -1)]\n",
    "        m_l += 1\n",
    "    cands = [c.replace(tokenizer.pad_token, '').strip() for c in cands]\n",
    "    return cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = dict(\n",
    "    enumerate([\n",
    "    'bert-base-uncased',  # 0\n",
    "    'albert-base-v2',  # 1\n",
    "    'roberta-base',  # 2\n",
    "    'google/electra-base-generator'  # 3\n",
    "    ])\n",
    ")\n",
    "model_path = model_dict[3]\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path)  # albert_model / bert_model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)  # albert_tokenizer / bert_tokenizer\n",
    "# fillmask = pipeline('fill-mask', model=model_path)\n",
    "# generator = pipeline('text-generation', model='gpt2')\n",
    "# mask_token = fillmask.tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4515a277be9e4ff1b9dbd338bd519416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DataCreator:\n",
    "    s_ENT = '[E]'\n",
    "    e_ENT = '[/E]'\n",
    "    # f_ENT = lambda x: f'[E]{x}[/E]'\n",
    "\n",
    "    def __init__(self, data_path, template_token_lengths=10, top_k=5, model_idx=3, simple_knowledge_tag=True):\n",
    "        self.data_path = data_path\n",
    "        self.exceptions = ['BalanceSheet', 'IncomeStatement', 'Ratios', 'CalendarOneYear']\n",
    "        self.times = ['year', 'quarter']\n",
    "        self.template_token_lengths = template_token_lengths\n",
    "        self.top_k = top_k\n",
    "        self.model_idx = model_idx\n",
    "        self.simple_knowledge_tag = simple_knowledge_tag\n",
    "        self.set_sparql()\n",
    "\n",
    "        self.f_ENT = lambda x: f'{self.s_ENT}{x}{self.e_ENT}'\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.progress_bar = tqdm()\n",
    "\n",
    "    def set_sparql(self):\n",
    "        self.sparql = SparqlHandler(self.data_path / 'AccountRDF.xml')\n",
    "\n",
    "        df_account = pd.read_csv(self.data_path / 'AccountName.csv', encoding='utf-8')\n",
    "        self.ACC_DICT = defaultdict(dict)\n",
    "        for _, row in df_account.iterrows():\n",
    "            acc = row['acc']\n",
    "            eng = row['acc_name_eng']\n",
    "            kor = row['acc_name_kor']\n",
    "            group = row['group']\n",
    "            self.ACC_DICT[acc]['kor_name'] = kor\n",
    "            self.ACC_DICT[acc]['eng_name'] = eng\n",
    "            self.ACC_DICT[acc]['group'] = group\n",
    "\n",
    "ds = DataCreator(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('CurrentAssets', {'kor_name': '유동자산', 'eng_name': 'Current Assets', 'group': 'BS-Value-3'}), ('CashAndCashEquivalents', {'kor_name': '현금및현금성자산', 'eng_name': 'Cash and Cash Equivalents', 'group': 'BS-Value-4'}), ('TradeAndOtherCurrentReceivables', {'kor_name': '매출채권', 'eng_name': 'Trade and Other Current Receivables', 'group': 'BS-Value-4'}), ('PrepaidExpenses', {'kor_name': '선급비용', 'eng_name': 'Prepaid Expenses', 'group': 'BS-Value-4'}), ('Inventories', {'kor_name': '재고자산', 'eng_name': 'Inventories', 'group': 'BS-Value-4'}), ('NoncurrentAssets', {'kor_name': '비유동자산', 'eng_name': 'Noncurrent Assets', 'group': 'BS-Value-3'}), ('PropertyPlantAndEquipment', {'kor_name': '유형자산', 'eng_name': 'Property Plant and Equipment', 'group': 'BS-Value-4'}), ('IntangibleAssets', {'kor_name': '무형자산', 'eng_name': 'Intangible Assets', 'group': 'BS-Value-4'}), ('AssetsAbstract', {'kor_name': '자산총계', 'eng_name': 'Assets', 'group': 'BS-Value-1'}), ('CurrentLiabilities', {'kor_name': '유동부채', 'eng_name': 'Current Liabilities', 'group': 'BS-Value-3'}), ('TradeAndOtherCurrentPayables', {'kor_name': '매입채무 및 기타유동채무', 'eng_name': 'Trade and Other Current Payables', 'group': 'BS-Value-4'}), ('ShortTermBorrowings', {'kor_name': '단기차입금', 'eng_name': 'Short Term Borrowings', 'group': 'BS-Value-4'}), ('AdvancesCustomers', {'kor_name': '선수금', 'eng_name': 'Advances Customers', 'group': 'BS-Value-4'}), ('NoncurrentLiabilities', {'kor_name': '비유동부채', 'eng_name': 'Noncurrent Liabilities', 'group': 'BS-Value-3'}), ('BondsIssued', {'kor_name': '사채', 'eng_name': 'Bonds Issued', 'group': 'BS-Value-4'}), ('LongTermBorrowings', {'kor_name': '장기차입금', 'eng_name': 'Long Term Borrowings', 'group': 'BS-Value-4'}), ('LiabilitiesAbstract', {'kor_name': '부채총계', 'eng_name': 'Liabilities', 'group': 'BS-Value-2'}), ('EquitiesAbstract', {'kor_name': '자본총계', 'eng_name': 'Equities', 'group': 'BS-Value-2'}), ('LiabilitiesAndEquities', {'kor_name': '부채와자본총계', 'eng_name': 'Liabilities and Equities', 'group': 'BS-Value-1'}), ('TradeReceivableTurnoverPeriod', {'kor_name': '매출채권 회전기간', 'eng_name': 'Trade Receivable Turnover Period', 'group': 'BS-Ratio-98'}), ('PrepaidExpensesTurnoverPeriod', {'kor_name': '선급비용 회전기간', 'eng_name': 'Prepaid Expenses Turnover Period', 'group': 'BS-Ratio-98'}), ('InventoriesTurnoverPeriod', {'kor_name': '재고자산 회전기간', 'eng_name': 'Inventories Turnover Period', 'group': 'BS-Ratio-98'}), ('TradePayablesTurnoverPeriod', {'kor_name': '매입채무 회전기간', 'eng_name': 'Trade Payables Turnover Period', 'group': 'BS-Ratio-98'}), ('AdvancesCustomersTurnoverPeriod', {'kor_name': '선수금 회전기간', 'eng_name': 'Advances Customers Turnover Period', 'group': 'BS-Ratio-98'}), ('Revenue', {'kor_name': '수익(매출액)', 'eng_name': 'Revenue', 'group': 'IS-Value-5'}), ('CostOfSales', {'kor_name': '매출원가', 'eng_name': 'Cost of Sales', 'group': 'IS-Value-5'}), ('GrossProfit', {'kor_name': '매출총이익', 'eng_name': 'Gross Profit', 'group': 'IS-Value-4'}), ('SellingGeneralAdministrativeExpenses', {'kor_name': '판매비와관리비', 'eng_name': 'Selling General Administrative Expenses', 'group': 'IS-Value-4'}), ('OperatingIncome', {'kor_name': '영업이익', 'eng_name': 'Operating Income', 'group': 'IS-Value-3'}), ('FinanceIncome', {'kor_name': '금융수익', 'eng_name': 'Finance Income', 'group': 'IS-Value-3'}), ('FinancialExpenses', {'kor_name': '금융비용', 'eng_name': 'Financial Expenses', 'group': 'IS-Value-3'}), ('ProfitBeforeTax', {'kor_name': '법인세비용차감전순이익(손실)', 'eng_name': 'Profit Before Tax', 'group': 'IS-Value-2'}), ('IncomeTaxExpense', {'kor_name': '법인세비용', 'eng_name': 'IncomeTax Expense', 'group': 'IS-Value-2'}), ('Profit', {'kor_name': '당기순이익(손실)', 'eng_name': 'Profit', 'group': 'IS-Value-1'}), ('CostOfSalesRatio', {'kor_name': '매출원가율', 'eng_name': 'Cost of Sales Ratio', 'group': 'IS-Ratio-98'}), ('SellingGeneralAdministrativeRatio', {'kor_name': '판관비율', 'eng_name': 'Selling General Administrative Ratio', 'group': 'IS-Ratio-98'}), ('SalesAndSellingGeneralAdministrativeRatio', {'kor_name': '매출 및 판관 비율', 'eng_name': 'Sales and Selling General Administrative Ratio', 'group': 'IS-Ratio-98'}), ('IncomeTaxRatio', {'kor_name': '법인세율', 'eng_name': 'Income Tax Ratio', 'group': 'IS-Ratio-98'}), ('ProfitRatio', {'kor_name': '순이익율', 'eng_name': 'Profit Ratio', 'group': 'IS-Ratio-98'}), ('IncomeStatement', {'kor_name': '손익계산서', 'eng_name': 'Income Statement', 'group': 'IS-Value-0'}), ('BalanceSheet', {'kor_name': '재무상태표', 'eng_name': 'Balance Sheet', 'group': 'BS-Value-0'}), ('CalendarOneYear', {'kor_name': '365 일', 'eng_name': '365 days', 'group': 'TIME-Value-99'}), ('Ratios', {'kor_name': '재무비율', 'eng_name': 'Finance Ratio', 'group': 'FS-Ratio-0'})])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.ACC_DICT.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e648e68adb3242559c31667d0ff59dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'f_ENT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2784/1183438320.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# skip fill mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf_ENT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_account\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m# fill mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f_ENT' is not defined"
     ]
    }
   ],
   "source": [
    "template_token_lengths = 4\n",
    "top_k = 5\n",
    "exceptions = ['BalanceSheet', 'IncomeStatement', 'Ratios', 'CalendarOneYear']\n",
    "data0 = []\n",
    "progress_bar = tqdm()\n",
    "\n",
    "model_dict = dict(\n",
    "    enumerate([\n",
    "    'bert-base-uncased',  # 0\n",
    "    'albert-base-v2',  # 1\n",
    "    'roberta-base',  # 2\n",
    "    'google/electra-base-generator'  # 3\n",
    "    ])\n",
    ")\n",
    "model_path = model_dict[3]\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path)  # albert_model / bert_model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)  # albert_tokenizer / bert_tokenizer\n",
    "\n",
    "for acc, dic in ACC_DICT.items():\n",
    "    if acc in exceptions:\n",
    "        continue\n",
    "    target_account = dic['eng_name'].lower()\n",
    "    knowledge, acc_type, _ = dic['group'].split('-')\n",
    "    target_entitiy = f'{knowledge}'\n",
    "    for l in range(1, template_token_lengths+1):\n",
    "        progress_bar.set_description(f'{target_account}, {target_entitiy}: length-{l}')\n",
    "\n",
    "        if l == 1:\n",
    "            # skip fill mask\n",
    "            sentences = [f_ENT(target_account)]\n",
    "        else:\n",
    "            # fill mask\n",
    "            acc_idxes = np.random.choice(range(l), size=(l,), replace=False)\n",
    "\n",
    "            cands = template_generator(\n",
    "                mask_token=mask_token, \n",
    "                max_length=l,\n",
    "                acc_idxes=acc_idxes,\n",
    "                target_account=target_account\n",
    "            )\n",
    "            sentences = get_aug_sequences(\n",
    "                tokenizer=tokenizer, \n",
    "                model=model, \n",
    "                cands=cands, \n",
    "                top_k=top_k, \n",
    "                max_length=l\n",
    "            )\n",
    "            sentences = [f'{f_ENT(target_account)}'.join(aug_sent.split(target_account)) for aug_sent in sentences]\n",
    "        for s in sentences:\n",
    "            entities = []\n",
    "            entities.append(get_entity(s, f_ENT(target_account), target_entitiy))\n",
    "            data0.append(\n",
    "                {'text': s, 'entities': sorted(entities, key=lambda x: x[0]), 'intent': 'None'} \n",
    "            )\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        if l == 3:\n",
    "            break\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 4\n",
    "acc_idxes = np.random.choice(range(l), size=(l,), replace=False)\n",
    "\n",
    "def template_generator(mask_token, max_length, acc_idxes, target_account):\n",
    "    preset = [mask_token] * max_length\n",
    "    candidates = []\n",
    "    for i in acc_idxes:\n",
    "        temp = deepcopy(preset)\n",
    "        temp[i] = target_account\n",
    "        candidates.append(' '.join(temp))\n",
    "    return candidates\n",
    "\n",
    "cands = template_generator(\n",
    "    mask_token=mask_token, \n",
    "    max_length=l,\n",
    "    acc_idxes=acc_idxes,\n",
    "    target_account=target_account\n",
    ")\n",
    "\n",
    "max_length = l\n",
    "number_of_mask_token = max_length-1\n",
    "m_l = 0\n",
    "while m_l < number_of_mask_token:\n",
    "    cands_tokens = [tokenizer.tokenize(c, truncation=True) for c in cands]\n",
    "    infer_indices = []\n",
    "    for c in cands_tokens:\n",
    "        mask_idx = list(np.arange(len(c))[np.array(c) == '[MASK]'])\n",
    "        choosed_idx = np.random.choice(mask_idx)\n",
    "        infer_indices.append(choosed_idx)\n",
    "    \n",
    "    inputs = tokenizer(cands, return_tensors='pt')\n",
    "    o = model(**inputs)[0][:, 1:-1]\n",
    "    _, sorted_idx = o.sort(dim=-1, descending=True)\n",
    "    predict_ids = sorted_idx[torch.arange(sorted_idx.size(0)), infer_indices, :top_k]\n",
    "\n",
    "    cands_tensors = inputs['input_ids'][:, 1:-1]\n",
    "    cands_tensors = cands_tensors.unsqueeze(1).expand(cands_tensors.size(0), top_k, cands_tensors.size(1)).clone()\n",
    "    cands_tensors[torch.arange(cands_tensors.size(0)), :, infer_indices] = predict_ids\n",
    "    cands = [tokenizer.decode(c) for c in cands_tensors.view(cands_tensors.size(0)*top_k, -1)]\n",
    "    if m_l == max_length-3:\n",
    "        break\n",
    "    m_l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 7])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TensorDataset(inputs['input_ids'], inputs['token_type_ids'], inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "batch_cands = []\n",
    "for i, x in enumerate(DataLoader(ds, batch_size=batch_size)):\n",
    "    batch_infer_indices = infer_indices[(i*batch_size):((i+1)*batch_size)]\n",
    "    o = model(input_ids=x[0], token_type_ids=x[1], attention_mask=x[2])[0][:, 1:-1]\n",
    "    _, sorted_idx = o.sort(dim=-1, descending=True)\n",
    "    predict_ids = sorted_idx[torch.arange(sorted_idx.size(0)), batch_infer_indices, :top_k]\n",
    "\n",
    "    cands_tensors = x[0][:, 1:-1]\n",
    "    cands_tensors = cands_tensors.unsqueeze(1).expand(cands_tensors.size(0), top_k, cands_tensors.size(1)).clone()\n",
    "    cands_tensors[torch.arange(cands_tensors.size(0)), :, batch_infer_indices] = predict_ids\n",
    "    batch_cands.extend([tokenizer.decode(c) for c in cands_tensors.view(cands_tensors.size(0)*top_k, -1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 2, 2, 2, 4, 4, 1, 1, 4, 4, 3, 3, 3, 3, 4, 4, 2, 2, 2]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what happens to the [E]sales and selling general administrative ratio[/E] when the [E]cost of sales ratio[/E] [E]decreases[/E] by [E]17 %[/E] in the [E]first quarter[/E]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remapping(text, ents):\n",
    "    splitted = []\n",
    "    text_copy = text[:]\n",
    "    e_prev = 0\n",
    "    for s, e, _ in ents:\n",
    "        new_s = s-e_prev\n",
    "        new_e = e-e_prev\n",
    "        splitted.append(text_copy[:new_s])\n",
    "        splitted.append(f_ENT(text_copy[new_s:new_e]))\n",
    "        text_copy = text_copy[new_e:]\n",
    "        e_prev = e\n",
    "    new_text = ''.join(splitted)\n",
    "    return new_text\n",
    "\n",
    "remapped_text = remapping(text, ents)\n",
    "remapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some Tokens are not properly assigned['O', 'O', 'O', 'O', 'O', '-', '-', '-', '-', '-', '-', '-', 'O', '-', '-', '-', '-', '-', '-', '-', 'O', '-', '-', '-', '-', '-', '-', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5040/3342560715.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbert_offset_mapping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlu_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremapped_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'offset_mapping'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlu_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset_mapping_to_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbert_offset_mapping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbiluo_to_iob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\simon\\Desktop\\Codes\\FSQA\\src\\nlu_utils.py\u001b[0m in \u001b[0;36moffset_mapping_to_tags\u001b[1;34m(self, offset_mapping, ents)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'-'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Some Tokens are not properly assigned'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mf'{labels}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Some Tokens are not properly assigned['O', 'O', 'O', 'O', 'O', '-', '-', '-', '-', '-', '-', '-', 'O', '-', '-', '-', '-', '-', '-', '-', 'O', '-', '-', '-', '-', '-', '-', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
     ]
    }
   ],
   "source": [
    "bert_tokens = nlu_tokenizer.bert_tokenize(remapped_text)\n",
    "\n",
    "bert_offset_mapping = nlu_tokenizer.bert(remapped_text, add_special_tokens=True, return_offsets_mapping=True)['offset_mapping']\n",
    "tags = nlu_tokenizer.offset_mapping_to_tags(offset_mapping=bert_offset_mapping, ents=ents)\n",
    "tags = biluo_to_iob(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 10, 11, 17, 20, 21, 24]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = [i for i, t in enumerate(tags) if t == 'O']\n",
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['what happens to the sales and selling general administrative ratio when the cost of sales ratio decreases below 17 % in the first quarter?',\n",
       " 'what happens to the sales and selling general administrative ratio when the cost of sales ratio decreases to 17 % in the first quarter?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = np.random.choice(masks)\n",
    "print(K)\n",
    "masked_sentence = \" \".join(bert_tokens[:K] + [mask_token] + bert_tokens[K+1:])\n",
    "predictions = fillmask(masked_sentence)\n",
    "augmented_sequences = [predictions[i]['sequence'] for i in range(3) if predictions[i]['sequence'] != text]\n",
    "for aug_s in augmented_sequences:\n",
    "    nlu_tokenizer.bert_tokenize(aug_s)\n",
    "augmented_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_offset_mapping = nlu_tokenizer.bert(text, add_special_tokens=False, return_offsets_mapping=True)['offset_mapping']\n",
    "bert_tokens = nlu_tokenizer.bert_tokenize(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens = nlu_tokenizer.spacy_tokenize(text)\n",
    "bert_encodes = nlu_tokenizer.bert(text, add_special_tokens=False, return_offsets_mapping=True)\n",
    "bert_offset_mapping = bert_encodes['offset_mapping']\n",
    "# bert_tokens = [nlu_tokenizer.bert_decode([i]) for i in bert_encodes['input_ids']]\n",
    "bert_tokens = nlu_tokenizer.bert_tokenize(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['was', \"n't\"], ['wasn', \"'\", 't'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokens, bert_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader validation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d010cbc4fcc4447a9009cade7aad8f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = data_module.create_dataset(data_module.train_data)\n",
    "\n",
    "all_tags = []\n",
    "with (data_path / 'trainset.jsonl').open('w', encoding='utf-8') as file:\n",
    "\n",
    "    restart_idx = 0\n",
    "    for i in tqdm(range(restart_idx, len(train_dataset)), total=len(train_dataset)-restart_idx):\n",
    "        item = train_dataset.__getitem__(i)\n",
    "        file.write(json.dumps(item) + '\\n')\n",
    "        all_tags.extend(item['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260aaf6ef8124981a4d58a38b467eab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_dataset = data_module.create_dataset(data_module.valid_data)\n",
    "restart_idx = 0\n",
    "for i in tqdm(range(restart_idx, len(valid_dataset)), total=len(valid_dataset)-restart_idx):\n",
    "    valid_dataset.__getitem__(i)\n",
    "    assert item['tags'].size(0) == 256, f\"tags_size={item['tags'].size()}\"\n",
    "    assert isinstance(item['intent'].tolist(), int), f\"intent_size={item['intent']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d7d321221443119b85f8baaf1ebfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3693 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = data_module.create_dataset(data_module.test_data)\n",
    "restart_idx = 0\n",
    "for i in tqdm(range(restart_idx, len(test_dataset)), total=len(test_dataset)-restart_idx):\n",
    "    test_dataset.__getitem__(i)\n",
    "    assert item['tags'].size(0) == 256, f\"tags_size={item['tags'].size()}\"\n",
    "    assert isinstance(item['intent'].tolist(), int), f\"intent_size={item['intent']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data_module.train_dataloader():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Debugging]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training import biluo_to_iob, offsets_to_biluo_tags, biluo_tags_to_offsets, iob_to_biluo\n",
    "from src.nlu_utils import NLUTokenizer\n",
    "\n",
    "nlu_tokenizer = NLUTokenizer(hugg_path='bert-base-uncased', spacy_path='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading: 100%|██████████| 14158/14158 [00:00<00:00, 101630.76it/s]\n",
      "loading: 100%|██████████| 14158/14158 [00:00<00:00, 221810.76it/s]\n",
      "loading: 100%|██████████| 14158/14158 [00:00<00:00, 90419.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from nlu_utils import NLUDataModule\n",
    "setting_path = src_path / 'setting_files'\n",
    "\n",
    "with (setting_path / 'train_settings.yml').open('r') as file:\n",
    "    settings = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "data_module_settings = settings['data_module']\n",
    "model_settings = settings['model']\n",
    "trainer_settings = settings['trainer']\n",
    "\n",
    "data_module = NLUDataModule(\n",
    "    train_path=data_path / data_module_settings['train_file'], \n",
    "    valid_path=data_path / data_module_settings['valid_file'],\n",
    "    test_path=data_path / data_module_settings['test_file'],\n",
    "    labels_path=data_path / data_module_settings['labels_file'],\n",
    "    batch_size=data_module_settings['batch_size'], \n",
    "    max_len=data_module_settings['max_len'],\n",
    "    num_workers=data_module_settings['num_workers'],\n",
    "    seed=settings['seed']\n",
    ")\n",
    "\n",
    "data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from collections import defaultdict\n",
    "from transformers import BertConfig, BertModel, BertTokenizerFast\n",
    "from torchcrf import CRF\n",
    "from nlu_utils import NLUTokenizer\n",
    "from spacy.training import biluo_to_iob\n",
    "\n",
    "setting_path = src_path / 'setting_files'\n",
    "\n",
    "with Path(data_path / 'labels_simple.json').open('r', encoding='utf-8') as file:\n",
    "    labels = json.load(file)\n",
    "\n",
    "tags2id = {v: k for k, v in enumerate(labels['tags'])}\n",
    "intents2id = {v: k for k, v in enumerate(labels['intent'])}\n",
    "\n",
    "with (setting_path / 'train_settings.yml').open('r') as file:\n",
    "        settings = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "data_module_settings = settings['data_module']\n",
    "model_settings = settings['model']\n",
    "trainer_settings = settings['trainer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from nlu_models import NLUModel\n",
    "\n",
    "hparams = {\n",
    "    'stage': model_settings['stage'],\n",
    "    'model_path': model_settings['model_path'], \n",
    "    'intent_size': len(data_module.intents2id), \n",
    "    'tags_size': len(data_module.tags2id), \n",
    "    'lr': model_settings['lr'],\n",
    "    'weight_decay_rate': model_settings['weight_decay_rate'],\n",
    "    'loss_type': model_settings['loss_type'],\n",
    "    'multigpu': True if trainer_settings['n_gpus'] > 1 else False,\n",
    "    'weight_dict': None\n",
    "}\n",
    "for k, v in model_settings['optim'].items():\n",
    "    hparams[f'optim_{k}'] = v\n",
    "for k, v in model_settings['schedular'].items():\n",
    "    hparams[f'schedular_{k}'] = v\n",
    "if model_settings['loss_type'] == 'focal':\n",
    "    for k, v in model_settings['focal'].items():\n",
    "        hparams[f'focal_{k}'] = v\n",
    "\n",
    "model = NLUModel(**hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_loader:\n",
    "    bert_inputs = {k: x[k] for k in ['input_ids', 'token_type_ids', 'attention_mask']}\n",
    "    tags = x['tags']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(**{k: v.to('cpu') for k, v in x.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\miniconda3\\envs\\venv\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py:415: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "o = model.forward_all(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "cfg = BertConfig()\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = NLUTokenizer() # BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "crf = CRF(num_tags=len(tags2id), batch_first=True)\n",
    "linear = nn.Linear(cfg.hidden_size, len(tags2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = {\"text\": \"the need to manage current assets \", \"entities\": [[19, 33, \"BS\"]], \"intent\": \"None\"}\n",
    "\n",
    "# text = x['text']\n",
    "# ents = x['entities']\n",
    "# intent = x['intent']\n",
    "\n",
    "# bert_offset_mapping = tokenizer.bert(text, add_special_tokens=False, return_offsets_mapping=True)['offset_mapping']\n",
    "# tags = tokenizer.offset_mapping_to_tags(offset_mapping=bert_offset_mapping, ents=ents)\n",
    "# tags = biluo_to_iob(tags)\n",
    "\n",
    "# bert_encodes = tokenizer(\n",
    "#     text, \n",
    "#     add_special_tokens=True, \n",
    "#     truncation=True, \n",
    "#     max_length=256\n",
    "# )\n",
    "# numeric_tags = list(map(tags2id.get, tags))\n",
    "# padded_tags = tokenizer.pad_tags(numeric_tags, pad_idx=tags2id.get('O'))\n",
    "# intent = intents2id.get(intent)\n",
    "\n",
    "# item = {k: [v] for k, v in bert_encodes.items()}\n",
    "# item['intent'] = intent\n",
    "# item['tags'] = [padded_tags]\n",
    "# inputs = {k: torch.as_tensor(v) for k, v in item.items()}\n",
    "# bert_inputs = {k: inputs[k] for k in ['input_ids', 'token_type_ids', 'attention_mask']}\n",
    "# tags = inputs['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "\n",
    "log_path = src_path / 'logs'\n",
    "checkpoint_path = src_path / 'checkpoints'\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=3)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.optim.lr_scheduler import _LRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_with_restarts_lr_lambda(self, current_step):    \n",
    "    if current_step < self.hparams.schedular_warmup_steps:\n",
    "        return float(current_step) / float(max(1, self.hparams.schedular_warmup_steps))\n",
    "    progress = float(current_step - self.hparams.schedular_warmup_steps) / float(max(1, self.num_training_steps - self.hparams.schedular_warmup_steps))\n",
    "    if progress >= 1.0:\n",
    "        return 0.0\n",
    "    return max(0.0, 0.5 * (1.0 + math.cos(math.pi * ((float(self.hparams.schedular_num_cycles) * progress) % 1.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cosineLrLmabda(_LRScheduler):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_lr_lambda(current_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 100\n",
    "t = 2500\n",
    "c = 3\n",
    "op = torch.optim.Adam(nn.Linear(1, 1).parameters())\n",
    "torch.optim.lr_scheduler.LambdaLR(optimizer, cosine_lr_lambda, -1)\n",
    "# a = get_scheduler('linear', op, w, t)\n",
    "get_cosine_with_hard_restarts_schedule_with_warmup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'get_linear_schedule_with_warmup.<locals>.lr_lambda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12188/3739601094.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'get_linear_schedule_with_warmup.<locals>.lr_lambda'"
     ]
    }
   ],
   "source": [
    "pickle.dumps(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []\n",
    "for i in range(t):\n",
    "    op.step()\n",
    "    a.step()\n",
    "    l = a.get_last_lr()\n",
    "    b.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c024f69c10>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.LambdaLR at 0x26c8c283d30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.optim.lr_scheduler.LambdaLR(torch.optim.Adam(nn.Linear(1, 1).parameters()), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model, datamodule=data_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab7f212329a491b497f27876271d03c022f2dd26760015eef69af619991238fd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
