{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "src_path = Path('.').absolute().parent\n",
    "data_path = src_path / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "import yaml\n",
    "import networkx as nx\n",
    "from src.ontology import OntologySystem\n",
    "\n",
    "with (src_path / 'setting_files' / 'app_settings.yml').open('r') as file:\n",
    "    settings = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "onto = OntologySystem(\n",
    "    acc_name_path=data_path / 'AccountName.csv', \n",
    "    rdf_path=data_path / 'AccountRDF.xml',\n",
    "    model_path=data_path / settings['ontology']['model']['model_name'],\n",
    "    kwargs_graph_drawer=settings['ontology']['graph_drawer']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for guessing masking tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "\n",
    "model_path = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Asking information based on fact and knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9f3b58dcca46b6b47efdc64cfc222d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9440/438226843.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0minputs_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mmasked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask_token_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmasked\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mlogits_top\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mn_top\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mprobs_top\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits_top\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m         outputs = self.bert(\n\u001b[0m\u001b[0;32m   1335\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         )\n\u001b[1;32m--> 996\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    581\u001b[0m                 )\n\u001b[0;32m    582\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    584\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     ):\n\u001b[1;32m--> 400\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             \u001b[0mkey_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mtranspose_for_scores\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[0mnew_x_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_x_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     def forward(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Question 1\n",
    "# what is the Cost of sales ratio in last year?\n",
    "threshold = 0.01\n",
    "exceptions = ['BalanceSheet', 'IncomeStatement', 'CalendarOneYear']\n",
    "times = ['year', 'quarter']\n",
    "sentence_format = \"[MASK] is the {} in the [MASK] {}?\"\n",
    "n_top = 15\n",
    "\n",
    "predicted_tokens_dict = defaultdict(set)\n",
    "progress_bar = tqdm(total=((len(ACC_DICT) - len(exceptions)) * len(times)))\n",
    "\n",
    "for acc, dic in ACC_DICT.items():\n",
    "    if acc in exceptions:\n",
    "        continue\n",
    "    account_name = dic['eng_name'].lower()\n",
    "    for t in times:\n",
    "        s = sentence_format.format(account_name, t.lower())\n",
    "\n",
    "        inputs = tokenizer(s, padding=True, truncation=True, return_token_type_ids=True, return_tensors='pt')\n",
    "        inputs_tensors = inputs['input_ids']\n",
    "        masked = inputs_tensors.eq(tokenizer.mask_token_id)\n",
    "        outputs = model(**inputs).logits[masked]\n",
    "        logits_top = outputs.argsort(descending=True)[:, :n_top]\n",
    "        probs_top = outputs.softmax(1).gather(1, logits_top)\n",
    "\n",
    "        for i, m in enumerate(probs_top >= threshold):\n",
    "            # tkns.append([k.item() for k in logits_top[i, m]])\n",
    "            for k in logits_top[i, m]:\n",
    "                tkn = tokenizer.decode(k)\n",
    "                if len(re.findall(r'(\\\")', tkn)) == 0:\n",
    "                    predicted_tokens_dict[f'[MASK]-{i}-{t}'].add(tkn)\n",
    "        \n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (data_path / 'tkns.csv').open('w') as file:\n",
    "    for k, v in predicted_tokens_dict.items():\n",
    "        print(','.join([k] + list(v)), file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "What if: Analysis based on fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge = 'BS'\n",
    "knowledge_query = onto.sparql.get_predefined_knowledge(knowledge=knowledge+'R')\n",
    "results = onto.sparql.query(knowledge_query)\n",
    "nx_graph = onto.get_nx_graph(results)\n",
    "sub_tree = nx.bfs_successors(nx_graph, source='BalanceSheet')\n",
    "sub_tree = dict(sub_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253e563a860c43589fa9ba385fae17aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question 2\n",
    "# what happens to the operating income when the cost of sales increases by 10% this year?\n",
    "\n",
    "threshold = 0.01\n",
    "exceptions = ['BalanceSheet', 'IncomeStatement', 'CalendarOneYear']\n",
    "times = ['year', 'quarter']\n",
    "# sentence_format = \"what [MASK] to the {} when the {} [MASK] by {} {} in the [MASK] {}?\"\n",
    "sentence_format = \"what will be the effect to {} if the {} [MASK] by {} {} in the [MASK] {}?\"\n",
    "n_top = 15\n",
    "successors = []\n",
    "predicted_tokens_dict = defaultdict(set)\n",
    "progress_bar = tqdm()\n",
    "\n",
    "for sub_acc, accs in sub_tree.items():\n",
    "    if sub_acc in exceptions:\n",
    "        continue\n",
    "    sub_acc_name = ACC_DICT[sub_acc]['eng_name'].lower()\n",
    "    successors.extend(accs)\n",
    "    for acc in successors:\n",
    "        account_name = ACC_DICT[acc]['eng_name'].lower()\n",
    "        for t in times:\n",
    "            s = sentence_format.format(\n",
    "                account_name, sub_acc_name, \n",
    "                np.random.randint(1, 50, (1,))[0], np.random.choice(['percent', '%']),\n",
    "                t.lower())\n",
    "\n",
    "            inputs = tokenizer(s, padding=True, truncation=True, return_token_type_ids=True, return_tensors='pt')\n",
    "            inputs_tensors = inputs['input_ids']\n",
    "            masked = inputs_tensors.eq(tokenizer.mask_token_id)\n",
    "            outputs = model(**inputs).logits[masked]\n",
    "            logits_top = outputs.argsort(descending=True)[:, :n_top]\n",
    "            probs_top = outputs.softmax(1).gather(1, logits_top)\n",
    "            for i, m in enumerate(probs_top >= threshold):\n",
    "                # tkns.append([k.item() for k in logits_top[i, m]])\n",
    "                for k in logits_top[i, m]:\n",
    "                    tkn = tokenizer.decode(k)\n",
    "                    if len(re.findall(r'(\\\")', tkn)) == 0:\n",
    "                        predicted_tokens_dict[f'[MASK]-{i}-{t}'].add(tkn)\n",
    "\n",
    "            progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (data_path / 'tkns.csv').open('w') as file:\n",
    "    for k, v in predicted_tokens_dict.items():\n",
    "        print(','.join([k] + list(v)), file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "What if: Forecasting with embedded ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5739072ae18481e96aad1149d926da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question 3\n",
    "# what will be our revenue in the 4th quarter?\n",
    "\n",
    "threshold = 0.01\n",
    "exceptions = ['BalanceSheet', 'IncomeStatement', 'CalendarOneYear']\n",
    "times = ['year', 'quarter']\n",
    "sentence_format = \"[MASK] will be the {} in the [MASK] {}?\"\n",
    "# sentence_format = \"how is the {} going to be in the [MASK] {}?\"\n",
    "n_top = 15\n",
    "\n",
    "predicted_tokens_dict = defaultdict(set)\n",
    "progress_bar = tqdm(total=((len(ACC_DICT) - len(exceptions)) * len(times)))\n",
    "\n",
    "for acc, dic in ACC_DICT.items():\n",
    "    if acc in exceptions:\n",
    "        continue\n",
    "    account_name = dic['eng_name'].lower()\n",
    "    for t in times:\n",
    "        s = sentence_format.format(account_name, t.lower())\n",
    "\n",
    "        inputs = tokenizer(s, padding=True, truncation=True, return_token_type_ids=True, return_tensors='pt')\n",
    "        inputs_tensors = inputs['input_ids']\n",
    "        masked = inputs_tensors.eq(tokenizer.mask_token_id)\n",
    "        outputs = model(**inputs).logits[masked]\n",
    "        logits_top = outputs.argsort(descending=True)[:, :n_top]\n",
    "        probs_top = outputs.softmax(1).gather(1, logits_top)\n",
    "\n",
    "        for i, m in enumerate(probs_top >= threshold):\n",
    "            # tkns.append([k.item() for k in logits_top[i, m]])\n",
    "            for k in logits_top[i, m]:\n",
    "                tkn = tokenizer.decode(k)\n",
    "                if len(re.findall(r'(\\\")', tkn)) == 0:\n",
    "                    predicted_tokens_dict[f'[MASK]-{i}-{t}'].add(tkn)\n",
    "        \n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (data_path / 'tkns.csv').open('w') as file:\n",
    "    for k, v in predicted_tokens_dict.items():\n",
    "        print(','.join([k] + list(v)), file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_DICT = onto.ACC_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need to define the future but past words cannot use in future\n",
    "df = pd.read_csv(data_path / 'AccountWords.csv', encoding='utf-8')\n",
    "\n",
    "format_dict = {\n",
    "    0: ['help'],\n",
    "    1: [\n",
    "        # what/how, target_account, [MASK] + year/quarter\n",
    "        \"{} is the {} in the {}?\",\n",
    "    ], \n",
    "    2: [\n",
    "        # target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "        \"what happens to the {} when the {} {} by {} in the {}?\",\n",
    "        # target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "        \"what will be the effect to {} if the {} {} by {} in the {}?\",\n",
    "        # reverse the relation\n",
    "        # subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "        \"when the {} {} by {} in the {}, what will happen to the {}?\",\n",
    "        # subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "        \"if the {} {} by {} in the {}, what will be the effect to {}?\"\n",
    "    ],\n",
    "    3: [\n",
    "        # what/how, target_account, [MASK] + year/quarter\n",
    "        \"{} will be the {} in the {}?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# TODO: maybe add the today's information after [SEP]?\n",
    "context = ['HELP', 'PAST', 'FUTURE']\n",
    "words = defaultdict(list)\n",
    "for typ in ['year', 'quarter', 'words']:\n",
    "    df_temp = df.loc[:, [typ, f'{typ}_tag', f'{typ}_desc']]\n",
    "    df_temp = df_temp.loc[~df_temp[typ].isna(), :]\n",
    "    for i, (w, t, desc) in df_temp.iterrows():\n",
    "        words[typ].append((w, t, desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: position 만들기\n",
    "# (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]})\n",
    "\n",
    "def get_entity(s, x, tag):\n",
    "    idx = s.index(x)\n",
    "    return (idx, idx+len(x), tag)\n",
    "\n",
    "threshold = 0.01\n",
    "exceptions = ['BalanceSheet', 'IncomeStatement', 'Ratios', 'CalendarOneYear']\n",
    "times = ['year', 'quarter']\n",
    "\n",
    "all_data = []\n",
    "s_ENT = '[E]'\n",
    "e_ENT = '[/E]'\n",
    "f_ENT = lambda x: f'{s_ENT}{x}{e_ENT}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "```python\n",
    "# what/how, target_account, [MASK] + year/quarter\n",
    "\"{} is the {} in the {}?\",\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d62a8032d3a44aeb598cf5dbf271493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trg_scenario = 1\n",
    "progress_bar = tqdm()\n",
    "for fmt in format_dict[trg_scenario]:\n",
    "    for acc, dic in ACC_DICT.items():\n",
    "        if acc in exceptions:\n",
    "            continue\n",
    "        target_account = dic['eng_name'].lower()\n",
    "        knowledge, *_ = dic['group'].split('-')\n",
    "        for t in times:\n",
    "            for t_word, t_tag, t_desc in words[t]:\n",
    "                if t_desc != 'FUTURE':  # only add not future word in first scenario\n",
    "                    entities = []\n",
    "\n",
    "                    s = fmt.format(\n",
    "                        np.random.choice(['what', 'how']),\n",
    "                        f_ENT(target_account), \n",
    "                        f_ENT(f'{t_word} {t}')\n",
    "                        )\n",
    "                    relation = [0, 0, 0]  # no_relation, order1, order2\n",
    "                    # entities\n",
    "                    ## target_account\n",
    "                    entities.append(get_entity(s, f_ENT(target_account), f'{knowledge}.{acc}'))\n",
    "                    ## MASK year/quarter\n",
    "                    entities.append(get_entity(s, f_ENT(f'{t_word} {t}'), t_tag))\n",
    "                    \n",
    "                    all_data.append(\n",
    "                        {'question': s, 'entities': entities, 'intent': 'PAST.value', 'relation': relation}\n",
    "                    )\n",
    "                    \n",
    "                    progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "```python\n",
    "# target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "\"what happens to the {} when the {} {} by {} in the {}?\"\n",
    "# target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "\"what will be the effect to {} if the {} {} by {} in the {}?\"\n",
    "# reverse the relation\n",
    "# subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "\"when the {} {} by {} in the {}, what will happen to the {}?\"\n",
    "# subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "\"if the {} {} by {} in the {}, what will be the effect to {}?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_role_dict(onto, knowledge):\n",
    "    knowledge_query = onto.sparql.get_predefined_knowledge(knowledge=knowledge)\n",
    "    sparql_results = onto.sparql.query(knowledge_query)\n",
    "    role_dict = defaultdict(list)\n",
    "    for s, p, o in sparql_results:\n",
    "        s, p, o = map(onto.graph_drawer.convert_to_string, [s, p, o])\n",
    "        if s == 'CalendarOneYear' or o == 'CalendarOneYear':\n",
    "            continue\n",
    "        if s not in role_dict[o]:\n",
    "            role_dict[o].append(s)\n",
    "        \n",
    "    return role_dict\n",
    "\n",
    "def process_successor(successors, role_dict, trg_acc, acc):\n",
    "    if role_dict.get(acc) is None:\n",
    "        # successors[trg_acc].extend(successors[acc])\n",
    "        return None\n",
    "    else:\n",
    "        accs = role_dict.get(acc)\n",
    "        if accs is not None:\n",
    "            successors[trg_acc].extend(accs)\n",
    "            for acc in accs:\n",
    "                process_successor(successors, role_dict, trg_acc, acc)\n",
    "\n",
    "def get_successor(onto, knowledge, exceptions=None):\n",
    "    role_dict = get_role_dict(onto, knowledge=knowledge)\n",
    "    successors = defaultdict(list)\n",
    "    for trg_acc in role_dict.keys():\n",
    "        if (exceptions is not None) and (trg_acc in exceptions):\n",
    "            continue\n",
    "        process_successor(successors, role_dict, trg_acc, trg_acc)\n",
    "    return successors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = ['BalanceSheet', 'IncomeStatement', 'Ratios', 'CalendarOneYear']\n",
    "\n",
    "bs_successors = get_successor(onto, 'BS', exceptions)\n",
    "is_successors = get_successor(onto, 'IS', exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60a0ad4f59b4bb39a354e031791af21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trg_scenario = 2\n",
    "progress_bar = tqdm()\n",
    "for idx_fmt, fmt in enumerate(format_dict[trg_scenario]):\n",
    "    for sub_tree in [bs_successors, is_successors]:\n",
    "        for trg_acc, successors in sub_tree.items():\n",
    "            if trg_acc in exceptions:\n",
    "                continue\n",
    "            target_account = ACC_DICT[trg_acc]['eng_name'].lower()\n",
    "            target_knowledge, *_ = ACC_DICT[trg_acc]['group'].split('-')\n",
    "            for sub_acc in successors:\n",
    "                subject_account = ACC_DICT[sub_acc]['eng_name'].lower()\n",
    "                subject_knowledge, *_ = ACC_DICT[trg_acc]['group'].split('-')\n",
    "                for apply_word, apply_tag, apply_desc in words['words']:\n",
    "                    for t in times:\n",
    "                        for t_word, t_tag, t_desc in words[t]:\n",
    "                            if t_desc != 'FUTURE':  # only add not future word in second scenario\n",
    "                                entities = []\n",
    "                                number = np.random.randint(1, 99)\n",
    "                                percent = np.random.choice(['percent', '%'])\n",
    "                                \n",
    "                                if idx_fmt in [0, 1]:\n",
    "                                    # target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "                                    s = fmt.format(\n",
    "                                        f_ENT(target_account), \n",
    "                                        f_ENT(subject_account), \n",
    "                                        f_ENT(apply_word), \n",
    "                                        f_ENT(f'{number} {percent}'),\n",
    "                                        f_ENT(f'{t_word} {t}')\n",
    "                                        )\n",
    "                                    relation = [1, 1, 2]\n",
    "                                else:\n",
    "                                    # subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "                                    s = fmt.format(\n",
    "                                        f_ENT(subject_account), \n",
    "                                        f_ENT(apply_word), \n",
    "                                        f_ENT(f'{number} {percent}'),\n",
    "                                        f_ENT(f'{t_word} {t}'),\n",
    "                                        f_ENT(target_account)\n",
    "                                        )\n",
    "                                    relation = [1, 2, 1]\n",
    "                                # entities\n",
    "                                ## target_account\n",
    "                                entities.append(get_entity(s, f_ENT(target_account), f'{target_knowledge}.{trg_acc}'))\n",
    "                                ## subject_account\n",
    "                                entities.append(get_entity(s, f_ENT(subject_account), f'{subject_knowledge}.{sub_acc}'))\n",
    "                                ## MASK apply words\n",
    "                                entities.append(get_entity(s, f_ENT(apply_word), apply_tag))\n",
    "                                ## percentages\n",
    "                                entities.append(get_entity(s, f_ENT(f'{number} {percent}'), 'PERCENT'))\n",
    "                                ## MASK year/quarter\n",
    "                                entities.append(get_entity(s, f_ENT(f'{t_word} {t}'), t_tag))\n",
    "                    \n",
    "                                all_data.append(\n",
    "                                    {'question': s, 'entities': sorted(entities, key=lambda x: x[0]), 'intent': 'IF.fact', 'relation': relation}\n",
    "                                )\n",
    "                                \n",
    "                                progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "```python\n",
    "# what/how, target_account, [MASK] + year/quarter\n",
    "\"{} will be the {} in the {}?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3413946f7f7143f8bed18d0b4d78cc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trg_scenario = 3\n",
    "progress_bar = tqdm()\n",
    "for fmt in format_dict[trg_scenario]:\n",
    "    for acc, dic in ACC_DICT.items():\n",
    "        if acc in exceptions:\n",
    "            continue\n",
    "        target_account = dic['eng_name'].lower()\n",
    "        knowledge, *_ = dic['group'].split('-')\n",
    "        for t in times:\n",
    "            for t_word, t_tag, t_desc in words[t]:\n",
    "                if t_desc != 'PAST':  # only add not past word in third scenario\n",
    "                    entities = []\n",
    "                    s = fmt.format(\n",
    "                        np.random.choice(['what', 'how']), \n",
    "                        f_ENT(target_account), \n",
    "                        f_ENT(f'{t_word} {t}')\n",
    "                        )\n",
    "                    relation = [0, 0, 0]\n",
    "                    # entities\n",
    "                    ## target_account\n",
    "                    entities.append(get_entity(s, f_ENT(target_account), f'{knowledge}.{acc}'))\n",
    "                    ## MASK year/quarter\n",
    "                    entities.append(get_entity(s, f_ENT(f'{t_word} {t}'), t_tag))\n",
    "                    \n",
    "                    all_data.append(\n",
    "                        {'question': s, 'entities': entities, 'intent': 'IF.forecast', 'relation': relation}\n",
    "                    )\n",
    "                    \n",
    "                    progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f46717b19df45a3881757deb04ecebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab361b886c14abc9926462613f5c9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "saving:   0%|          | 0/211587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "special_len = len(s_ENT)+len(e_ENT)\n",
    "\n",
    "for k, x in tqdm(enumerate(all_data), total=len(all_data)):\n",
    "    all_data[k]['question'] = x['question'].replace(s_ENT, '').replace(e_ENT, '')\n",
    "    for i, (s, e, ent) in enumerate(x['entities']):\n",
    "        new_s = s-i*special_len\n",
    "        new_e = new_s+(e-s)-special_len\n",
    "        all_data[k]['entities'][i] = (new_s, new_e, ent)\n",
    "\n",
    "with (data_path / 'all_data.jsonl').open('w', encoding='utf-8') as file:\n",
    "    for line in tqdm(all_data, total=len(all_data), desc='saving'):\n",
    "        file.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "class NLUTokenizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        hugg_path='bert-base-uncased', \n",
    "        spacy_path='en_core_web_sm'\n",
    "    ):\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(hugg_path)\n",
    "        self.spacy_nlp = spacy.load(spacy_path)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text)\n",
    "\n",
    "    def decode(self, token_ids, **kwargs):\n",
    "        return self.tokenizer.decode(token_ids, **kwargs)\n",
    "\n",
    "\n",
    "    def __call__(self, text, **kwargs):\n",
    "        return self.tokenizer(text, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def offsets_to_iob_tags(cls, encodes, ents, get_acc_relation=False):\n",
    "        \"\"\"\n",
    "        ```\n",
    "        IOB SCHEME\n",
    "        I - Token is inside an entity.\n",
    "        O - Token is outside an entity.\n",
    "        B - Token is the beginning of an entity.\n",
    "\n",
    "        BILUO SCHEME\n",
    "        B - Token is the beginning of a multi-token entity.\n",
    "        I - Token is inside a multi-token entity.\n",
    "        L - Token is the last token of a multi-token entity.\n",
    "        U - Token is a single-token unit entity.\n",
    "        O - Token is outside an entity.\n",
    "        ```\n",
    "        method: IOB SCHEME\n",
    "        modified from https://github.com/explosion/spaCy/blob/9d63dfacfc85e7cd6db7190bd742dfe240205de5/spacy/training/iob_utils.py#L63\n",
    "\n",
    "        encodes: batch encodes from huggingface TokenizerFast\n",
    "        ents: entities with start & end characters in sentences + entity\n",
    "        \"\"\"\n",
    "        acc_relation = list()\n",
    "\n",
    "        starts, ends = dict(), dict()\n",
    "        for tkn_idx, (s_idx, e_idx) in enumerate(encodes['offset_mapping']):\n",
    "            if s_idx == e_idx == 0:\n",
    "                continue\n",
    "            starts[s_idx] = tkn_idx\n",
    "            ends[e_idx] = tkn_idx\n",
    "        \n",
    "        char_in_ents = {}\n",
    "        labels = ['-'] * len(encodes['input_ids'])\n",
    "        for s_char, e_char, ent in ents:\n",
    "            if not ent:\n",
    "                for s in starts:\n",
    "                    labels[starts[s]] = 'O'\n",
    "            else:\n",
    "                for char_idx in range(s_char, e_char):\n",
    "                    if char_idx in char_in_ents.keys():\n",
    "                        raise ValueError(f'Trying to Overlapping same tokens: {char_in_ents[char_idx]} / {(s_char, e_char, ent)}')\n",
    "                    char_in_ents[char_idx] = (s_char, e_char, ent)\n",
    "                s_token = starts.get(s_char)\n",
    "                e_token = ends.get(e_char)\n",
    "\n",
    "                if s_token is not None and e_token is not None:\n",
    "                    labels[s_token] = f'B-{ent}'\n",
    "                    # add relation\n",
    "                    if get_acc_relation and len(ent.split('.')) > 1:\n",
    "                        acc_relation.append((s_token, e_token+1))\n",
    "\n",
    "                    for i in range(s_token + 1, e_token+1):\n",
    "                        labels[i] = f'I-{ent}'\n",
    "                        \n",
    "        entity_chars = set()\n",
    "        for s_char, e_char, ent in ents:\n",
    "            for i in range(s_char, e_char):\n",
    "                entity_chars.add(i)\n",
    "        for token_idx, (s, e) in enumerate(encodes['offset_mapping']):\n",
    "            for i in range(s, e):\n",
    "                if i in entity_chars:\n",
    "                    break\n",
    "            else:\n",
    "                labels[token_idx] = 'O'\n",
    "        if '-' in labels:\n",
    "            raise ValueError('Some Tokens are not properly assigned' + f'{labels}')\n",
    "\n",
    "        return labels, acc_relation\n",
    "\n",
    "    def pad_tags(self, input_ids, tags, pad_idx:int=-100):\n",
    "        padded_tags = [pad_idx] * len(input_ids)\n",
    "        j = 0\n",
    "        for i, tkn_id in enumerate(input_ids):\n",
    "            if tkn_id in self.tokenizer.all_special_ids:\n",
    "                continue\n",
    "            padded_tags[i] = tags[j]\n",
    "            j += 1\n",
    "        return padded_tags\n",
    "\n",
    "    # def get_labels(self, intent, input_ids, tags, pad_idx:int=-100):\n",
    "    #     labels = self.pad_tags(input_ids, tags, pad_idx)\n",
    "    #     labels[0] = intent\n",
    "    #     return labels\n",
    "\n",
    "\n",
    "nlu_tokenizer = NLUTokenizer(hugg_path='bert-base-uncased', spacy_path='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277da879b3364883bf45ab0e624d7264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading:   0%|          | 0/211587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2060/691859383.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mall_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'loading'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mall_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\venv\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[1;31m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# NOQA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "with (data_path / 'all_data.jsonl').open('r', encoding='utf-8') as file:\n",
    "    data = file.readlines()\n",
    "    all_data = []\n",
    "    for line in tqdm(data, total=len(data), desc='loading'):\n",
    "        all_data.append(json.loads(line))\n",
    "\n",
    "processed_data = []\n",
    "pad_id = nlu_tokenizer.tokenizer.pad_token_type_id\n",
    "for k, x in tqdm(enumerate(all_data), total=len(all_data), desc='processing data'):\n",
    "    encodes = nlu_tokenizer(text=x['question'], add_special_tokens=False, return_offsets_mapping=True)\n",
    "    has_relation = x['relation'][0]\n",
    "    tags, acc_relation = nlu_tokenizer.offsets_to_iob_tags(encodes, ents=x['entities'], get_acc_relation=has_relation)\n",
    "    if acc_relation:\n",
    "        # if there is relation process the coordinates of tokens\n",
    "        # target: 1 / subject: 2\n",
    "        # relation = [has_relation, target_coor, subject_coor]\n",
    "        # network will be guessing has_relation,  \n",
    "        a, b = x['relation'][1:]\n",
    "        input_ids = encodes['input_ids']\n",
    "        if a == 1 and b == 2:\n",
    "            s_trg, e_trg = acc_relation[0]\n",
    "            s_sub, e_sub = acc_relation[1]\n",
    "        elif a == 2 and b == 1:\n",
    "            s_trg, e_trg = acc_relation[1]\n",
    "            s_sub, e_sub = acc_relation[0]\n",
    "        # plus 1 for add cls token in the front of sentences\n",
    "        target_relation = (s_trg+1, e_trg+1)\n",
    "        subject_relation = (s_sub+1, e_sub+1)\n",
    "        relation = [has_relation, target_relation, subject_relation]\n",
    "    else:\n",
    "        relation = [has_relation, (0,0), (0,0)]\n",
    "    \n",
    "    processed_data.append((x['question'], tags, x['intent'], relation))\n",
    "\n",
    "\n",
    "with (data_path / 'all_data_processed.pickle').open('wb') as file:\n",
    "    pickle.dump(processed_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5041d56ee12c4ad59ef4a82faa200ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spliting data:   0%|          | 0/211587 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "seed=777\n",
    "with (data_path / 'all_data_processed.pickle').open('rb') as file:\n",
    "    all_data = pickle.load(file)\n",
    "\n",
    "questions, tags, intents, relations = list(zip(*all_data))\n",
    "# split to train & test\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=seed)\n",
    "train_idx, test_idx = list(*splitter.split(questions, intents))\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "tags_set = set()\n",
    "for idx in tqdm(range(len(questions)), total=len(questions), desc='spliting data'):\n",
    "    # process tags and intents\n",
    "    data = (questions[idx], tags[idx], intents[idx], relations[idx])\n",
    "\n",
    "    for t in tags[idx]:\n",
    "        tags_set.add(t)\n",
    "\n",
    "    if idx in train_idx:\n",
    "        train_data.append(data)\n",
    "    elif idx in test_idx:\n",
    "        test_data.append(data)\n",
    "    else:\n",
    "        raise ValueError(\"Index Error\")\n",
    "\n",
    "intents2id = {'None': 0}\n",
    "for intent in set(intents):\n",
    "    if intents2id.get(intent) is None:\n",
    "        intents2id[intent] = len(intents2id)\n",
    "\n",
    "tags2id = {'[PAD]': 0, 'O': 1}\n",
    "for t in tags_set:\n",
    "    if tags2id.get(t) is None:\n",
    "        tags2id[t] = len(tags2id)\n",
    "\n",
    "with (data_path / 'all_data_splitted.pickle').open('wb') as file:\n",
    "    pickle.dump({\n",
    "        'train': train_data, \n",
    "        'test': test_data, \n",
    "        }, file)\n",
    "\n",
    "with (data_path / 'all_data_ids.pickle').open('wb') as file:\n",
    "    pickle.dump({\n",
    "        'tags2id': tags2id, \n",
    "        'intents2id': intents2id\n",
    "        }, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning\n",
    "\n",
    "- Entities\n",
    "- Entities Relation (subject, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "class NLUDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, data, \n",
    "        tags2id=None, \n",
    "        intents2id=None, \n",
    "        hugg_path='bert-base-uncased', \n",
    "        spacy_path='en_core_web_sm', \n",
    "        max_len=128,\n",
    "    ):\n",
    "        self.questions, self.tags, self.intents, self.relations = list(zip(*data))\n",
    "        self.tokenizer = NLUTokenizer(hugg_path, spacy_path)\n",
    "        # question, entities, intent\n",
    "        self.tags2id = tags2id\n",
    "        self.intents2id = intents2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        question = self.questions[index]\n",
    "        tags = list(map(self.tags2id.get, self.tags[index]))\n",
    "        intent = self.intents2id.get(self.intents[index])\n",
    "        relation = self.relations[index]\n",
    "\n",
    "        encodes = self.tokenizer(\n",
    "            question, \n",
    "            return_offsets_mapping=False,\n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "        )\n",
    "        # labels = intent + tags\n",
    "        tags = self.tokenizer.pad_tags(\n",
    "            input_ids=encodes['input_ids'], \n",
    "            tags=tags, \n",
    "            pad_idx=0,\n",
    "        )\n",
    "\n",
    "        item = {k: torch.as_tensor(v) for k, v in encodes.items()}\n",
    "\n",
    "        item['intent'] =  torch.as_tensor(intent)\n",
    "        item['tags'] = torch.as_tensor(tags)\n",
    "        item['has_relation'] = torch.as_tensor(relation[0])\n",
    "        item['target_relation'] = torch.as_tensor(relation[1])\n",
    "        item['subject_relation'] = torch.as_tensor(relation[2])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "\n",
    "class NLUDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, data_path:Path, ids_path:Path,\n",
    "        batch_size:int=32, \n",
    "        max_len:int=128,\n",
    "        test_size=0.1,\n",
    "        num_workers=4,\n",
    "        seed=777\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.ids_path = ids_path\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.test_size = test_size\n",
    "        self.seed = seed\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        with Path(self.data_path).open('rb') as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        with Path(self.ids_path).open('rb') as file:\n",
    "            ids = pickle.load(file)\n",
    "\n",
    "        self.train_data = data['train']\n",
    "        self.test_data = data['test']\n",
    "        self.tags2id = ids['tags2id']\n",
    "        self.intents2id = ids['intents2id']\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = NLUDataset(\n",
    "            self.train_data, \n",
    "            tags2id=self.tags2id, \n",
    "            intents2id=self.intents2id,\n",
    "            max_len=self.max_len\n",
    "        )\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = NLUDataset(\n",
    "            self.test_data, \n",
    "            tags2id=self.tags2id, \n",
    "            intents2id=self.intents2id,\n",
    "            max_len=self.max_len\n",
    "        )\n",
    "        return DataLoader(val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "data_module = NLUDataModule(\n",
    "    data_path=data_path / 'all_data_splitted.pickle',\n",
    "    ids_path=data_path / 'all_data_ids.pickle',\n",
    "    batch_size=64, \n",
    "    max_len=max_len\n",
    ")\n",
    "data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# train_loader = data_module.train_dataloader()\n",
    "\n",
    "# dist = Counter() \n",
    "# for x in tqdm(train_loader, total=len(train_loader)):\n",
    "#     dist.update(list(x['intent'].numpy()))\n",
    "\n",
    "# test_loader = data_module.val_dataloader()\n",
    "\n",
    "# test_dist = Counter() \n",
    "# for x in tqdm(test_loader, total=len(test_loader)):\n",
    "#     test_dist.update(list(x['intent'].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import BertForTokenClassification, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"from https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert/modeling_bert.py#L627\"\"\"\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class RelationNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        \"\"\"output_size = max_len*4 + 1 (has_relation) \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.relation_net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, output_size*4+1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        o = self.relation_net(x)\n",
    "        has_relation = o[:, 0:1].squeeze(-1).contiguous()\n",
    "        relations = o[:, 1:]\n",
    "        s_target, e_target, s_subject, e_subject = map(lambda x: x.squeeze(-1).contiguous(), relations.split(self.output_size, dim=-1))\n",
    "        return has_relation, s_target, e_target, s_subject, e_subject\n",
    "\n",
    "class NLUModel(pl.LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        # self.hparams: model_path, intent_size, tags_size, max_len\n",
    "        self.outputs_keys = ['tags', 'intent', 'has_relation', 's_target', 'e_target', 's_subject', 'e_subject']\n",
    "        # Networks\n",
    "        cfg = BertConfig()\n",
    "        self.bert_ner = BertForTokenClassification.from_pretrained(self.hparams.model_path, num_labels=self.hparams.tags_size)\n",
    "        self.bert_pooler = BertPooler(cfg)\n",
    "        self.intent_network = nn.Linear(cfg.hidden_size, self.hparams.intent_size)\n",
    "        self.relation_network = RelationNetwork(cfg.hidden_size, self.hparams.max_len)\n",
    "        \n",
    "        # losses\n",
    "        if self.hparams.stage == 'train':\n",
    "            self.losses = {\n",
    "                'bce': nn.BCEWithLogitsLoss(),\n",
    "                'ce': nn.CrossEntropyLoss()\n",
    "            }\n",
    "            # metrics\n",
    "            self.metrics = {\n",
    "                'train_': self.create_metrics(prefix='train_'),\n",
    "                'val_': self.create_metrics(prefix='val_')\n",
    "            }\n",
    "    def contiguous(self, x):\n",
    "        return x.squeeze(-1).contiguous()\n",
    "\n",
    "    def create_metrics(self, prefix='train_'):\n",
    "        m = dict()\n",
    "        metrics = torchmetrics.MetricCollection([torchmetrics.Accuracy(), torchmetrics.Precision(), torchmetrics.Recall()])\n",
    "        # num_classes = {\n",
    "        #     'intent': self.hparams.intent_size, \n",
    "        #     'tags': self.hparams.tags_size,\n",
    "        #     'has_relation': 2,\n",
    "        #     's_target': self.hparams.max_len,\n",
    "        #     'e_target': self.hparams.max_len, \n",
    "        #     's_subject': self.hparams.max_len, \n",
    "        #     'e_subject': self.hparams.max_len\n",
    "        # }\n",
    "        for k in self.outputs_keys:\n",
    "            m[k] = metrics.clone(prefix+k+'_')\n",
    "        return m\n",
    "\n",
    "    def _forward_bert(self, input_ids, token_type_ids, attention_mask):\n",
    "        outputs = self.bert_ner.bert(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "    def _forward_tags(self, last_hidden_state):\n",
    "        tags_outputs = self.bert_ner.dropout(last_hidden_state)\n",
    "        tags_logits = self.bert_ner.classifier(tags_outputs)\n",
    "        # intent\n",
    "        return tags_logits.view(-1, self.hparams.tags_size)\n",
    "\n",
    "    def _forward_intent(self, pooled_outputs):\n",
    "        intent_logits = self.intent_network(pooled_outputs)\n",
    "        return intent_logits\n",
    "\n",
    "    def _forward_relation(self, pooled_outputs):\n",
    "        # pooled_outputs: (B, max_len, 768)\n",
    "        # has_relation: (B, )\n",
    "        # s_target, e_target, s_subject, e_subject: (B, max_len)\n",
    "        has_relation_logits, s_target_logits, e_target_logits, s_subject_logits, e_subject_logits = \\\n",
    "            self.relation_network(pooled_outputs)\n",
    "        return has_relation_logits, s_target_logits, e_target_logits, s_subject_logits, e_subject_logits\n",
    "\n",
    "    # def _get_relation_inputs(self, last_hidden_state, relation):\n",
    "    #     x = torch.stack([last_hidden_state[i, s:e].mean(0) for i, (s, e) in enumerate(relation)])\n",
    "    #     return x \n",
    "\n",
    "    # def _forward_relation(self, last_hidden_state, target_relation, subject_relation):\n",
    "    #     target_inputs = self._get_relation_inputs(last_hidden_state, target_relation)\n",
    "    #     subject_inputs = self._get_relation_inputs(last_hidden_state, subject_relation)\n",
    "    #     relation_inputs = torch.concat([last_hidden_state[:, 0], target_inputs, subject_inputs], dim=1)\n",
    "    #     has_relation_logits, s_target_logits, e_target_logits, s_subject_logits, e_subject_logits = self.relation_network(relation_inputs)\n",
    "    #     return has_relation_logits, s_target_logits, e_target_logits, s_subject_logits, e_subject_logits\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        # tags\n",
    "        batch_size = input_ids.size(0)\n",
    "        last_hidden_state = self._forward_bert(input_ids, token_type_ids, attention_mask)\n",
    "        tags_logits = self._forward_tags(last_hidden_state)\n",
    "\n",
    "        # intent\n",
    "        pooled_outputs = self.bert_pooler(last_hidden_state)\n",
    "        intent_logits = self._forward_intent(pooled_outputs)\n",
    "        # relation\n",
    "        has_relation_logits, s_target_logits, e_target_logits, s_subject_logits, e_subject_logits = \\\n",
    "            self._forward_relation(pooled_outputs)\n",
    "\n",
    "        return {\n",
    "            'tags': tags_logits,                       # (B*max_len, tags_size)\n",
    "            'intent': intent_logits,                   # (B, intent_size)\n",
    "            'has_relation': has_relation_logits,       # (B, )\n",
    "            's_target': s_target_logits,               # (B, max_len)\n",
    "            'e_target': e_target_logits,               # (B, max_len)\n",
    "            's_subject': s_subject_logits,             # (B, max_len)\n",
    "            'e_subject': e_subject_logits              # (B, max_len)\n",
    "        }\n",
    "\n",
    "    def forward_all(self, batch, prefix='train_'):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch['input_ids'], \n",
    "            token_type_ids=batch['token_type_ids'], \n",
    "            attention_mask=batch['attention_mask'], \n",
    "        )\n",
    "        s_target, e_target = map(self.contiguous, batch['target_relation'].split(1, dim=-1))\n",
    "        s_subject, e_subject = map(self.contiguous, batch['subject_relation'].split(1, dim=-1))\n",
    "        targets = {\n",
    "            'tags': batch['tags'].view(-1),         # (B*max_len, )\n",
    "            'intent': batch['intent'],              # (B, )\n",
    "            'has_relation': batch['has_relation'],  # (B, )\n",
    "            's_target': s_target,                   # (B, )\n",
    "            'e_target': e_target,                   # (B, )\n",
    "            's_subject': s_subject,                 # (B, )\n",
    "            'e_subject': e_subject                  # (B, )\n",
    "        }\n",
    "        loss = self.cal_loss(outputs, targets)\n",
    "        self.log(f'{prefix}loss', loss.item())\n",
    "        # logging\n",
    "        self.cal_metrics(outputs, targets, prefix=prefix)\n",
    "        return loss\n",
    "\n",
    "    def cal_loss(self, outputs, targets):\n",
    "        has_relation_loss = self.loss['bce'](outputs['has_relation'], targets['tags'].float())\n",
    "\n",
    "        tags_loss = self.loss['ce'](outputs['tags'], targets['tags'])\n",
    "        intent_loss = self.loss['ce'](outputs['intent'], targets['intent'])\n",
    "        s_target_loss = self.loss['ce'](outputs['s_target'], targets['s_target'])\n",
    "        e_target_loss = self.loss['ce'](outputs['e_target'], targets['e_target'])\n",
    "        s_subject_loss = self.loss['ce'](outputs['s_subject'], targets['s_subject'])\n",
    "        e_subject_loss = self.loss['ce'](outputs['e_subject'], targets['e_subject'])\n",
    "\n",
    "        return tags_loss + intent_loss + s_target_loss + e_target_loss + s_subject_loss + e_subject_loss + has_relation_loss\n",
    "\n",
    "    def cal_metrics(self, outputs, targets, prefix='train_'):\n",
    "        outputs_metrics = defaultdict()\n",
    "        for k in self.outputs_keys:\n",
    "            for k_sub, v in self.metrics[prefix][k](outputs[k], targets[k]).items():\n",
    "                outputs_metrics[k_sub] = v\n",
    "        self.log_dict(outputs_metrics)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.forward_all(batch, prefix='train_')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):   \n",
    "        loss = self.forward_all(batch, prefix='val_')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        return optimizer\n",
    "\n",
    "    def predict(self, input_ids, token_type_ids, attention_mask):\n",
    "        outputs = self.forward(input_ids, token_type_ids, attention_mask)\n",
    "        predicts = self._predict_from_outputs(outputs)\n",
    "        return predicts\n",
    "\n",
    "    def _predict_from_outputs(self, outputs):\n",
    "        predicts = {k: outputs[k].argmax(-1) for k in ['tags', 'intent', 's_target', 'e_target', 's_subject', 'e_subject']}\n",
    "        predicts['has_relation'] = (outputs['has_relation'].sigmoid() >= 0.5).byte()\n",
    "        return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "with Path(data_path / 'all_data_ids.pickle').open('rb') as file:\n",
    "    ids = pickle.load(file)\n",
    "tags2id = ids['tags2id']\n",
    "intents2id = ids['intents2id']\n",
    "\n",
    "hparams = {\n",
    "    'stage': 'train',\n",
    "    'model_path': 'bert-base-uncased', \n",
    "    'intent_size': len(intents2id), \n",
    "    'tags_size': len(tags2id), \n",
    "    'max_len': max_len,\n",
    "    'lr': 1e-3,\n",
    "    'load_path': None\n",
    "}\n",
    "\n",
    "model = NLUModel(**hparams)\n",
    "\n",
    "data_module = NLUDataModule(\n",
    "    data_path=data_path / 'all_data_splitted.pickle',\n",
    "    ids_path=data_path / 'all_data_ids.pickle',\n",
    "    batch_size=64, \n",
    "    max_len=max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/simon/Desktop/Codes/FSQA')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "\n",
    "log_path = src_path / 'logs'\n",
    "checkpoint_path = src_path / 'checkpoints'\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=str(log_path), name=\"NLU\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=str(checkpoint_path), \n",
    "    save_top_k=2,\n",
    "    monitor='val_loss'\n",
    ")\n",
    "progress_callback = TQDMProgressBar(refresh_rate=20)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=3, \n",
    "    logger=logger, \n",
    "    callbacks=[checkpoint_callback, progress_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                       | Params\n",
      "----------------------------------------------------------------\n",
      "0 | bert_ner         | BertForTokenClassification | 108 M \n",
      "1 | bert_pooler      | BertPooler                 | 590 K \n",
      "2 | intent_network   | Linear                     | 3.1 K \n",
      "3 | relation_network | RelationNetwork            | 789 K \n",
      "----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "441.356   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model, datamodule=data_module\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TQDMProgressBar(refresh_rate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k: x[k] for k in ['input_ids', 'token_type_ids', 'attention_mask']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([64, 64])\n",
      "token_type_ids torch.Size([64, 64])\n",
      "attention_mask torch.Size([64, 64])\n",
      "intent torch.Size([64])\n",
      "tags torch.Size([64, 64])\n",
      "has_relation torch.Size([64])\n",
      "target_relation torch.Size([64, 2])\n",
      "subject_relation torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "for k, v in x.items():\n",
    "    print(k, v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags torch.Size([4096, 83])\n",
      "intent torch.Size([64, 4])\n",
      "has_relation torch.Size([64])\n",
      "s_target torch.Size([64, 64])\n",
      "e_target torch.Size([64, 64])\n",
      "s_subject torch.Size([64, 64])\n",
      "e_subject torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "for k, v in o.items():\n",
    "    print(k, v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = model.predict(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_target, e_target = map(model.contiguous, x['target_relation'].split(1, dim=-1))\n",
    "s_subject, e_subject = map(model.contiguous, x['subject_relation'].split(1, dim=-1))\n",
    "t = {\n",
    "    'tags': x['tags'].view(-1), \n",
    "    'intent': x['intent'],\n",
    "    'has_relation': x['has_relation'],\n",
    "    's_target': s_target,\n",
    "    'e_target': e_target,\n",
    "    's_subject': s_subject,\n",
    "    'e_subject': e_subject\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096, 83]), torch.Size([4096]))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o['tags'].shape, t['tags'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = defaultdict()\n",
    "for k in model.outputs_keys:\n",
    "    for k_sub, v in model.metrics['train_'][k](o[k], t[k]).items():\n",
    "        a[k_sub] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'train_tags_Accuracy': tensor(0.0020),\n",
       "             'train_tags_Precision': tensor(0.0020),\n",
       "             'train_tags_Recall': tensor(0.0020),\n",
       "             'train_intent_Accuracy': tensor(0.),\n",
       "             'train_intent_Precision': tensor(0.),\n",
       "             'train_intent_Recall': tensor(0.),\n",
       "             'train_has_relation_Accuracy': tensor(0.4531),\n",
       "             'train_has_relation_Precision': tensor(1.),\n",
       "             'train_has_relation_Recall': tensor(0.4531),\n",
       "             'train_s_target_Accuracy': tensor(0.),\n",
       "             'train_s_target_Precision': tensor(0.),\n",
       "             'train_s_target_Recall': tensor(0.),\n",
       "             'train_e_target_Accuracy': tensor(0.),\n",
       "             'train_e_target_Precision': tensor(0.),\n",
       "             'train_e_target_Recall': tensor(0.),\n",
       "             'train_s_subject_Accuracy': tensor(0.),\n",
       "             'train_s_subject_Precision': tensor(0.),\n",
       "             'train_s_subject_Recall': tensor(0.),\n",
       "             'train_e_subject_Accuracy': tensor(0.),\n",
       "             'train_e_subject_Precision': tensor(0.),\n",
       "             'train_e_subject_Recall': tensor(0.)})"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_tagsAccuracy': tensor(0.0044),\n",
       " 'train_tagsPrecision': tensor(0.0044),\n",
       " 'train_tagsRecall': tensor(0.0044)}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics['train_']['tags'](o['tags'], t['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(o['has_relation'].sigmoid() >= 0.5).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] what happens to the current assets when the inventories drop by 97 % in the 1st quarter? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlu_tokenizer.decode(x['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] if the revenue decreases by 56 % in the second quarter, what will be the effect to cost of sales ratio? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlu_tokenizer.decode(x['input_ids'][-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  5, 20,  5, 18,  5, 18,  5,  7,  5, 20, 19,  7, 25,  5,  7,  5, 21,\n",
       "         5,  7,  5,  7,  5, 19,  5, 19, 19, 21,  7,  5, 25,  5, 20,  5, 20,  7,\n",
       "         7, 20, 21, 21, 22, 19,  5, 20, 19,  5, 18, 21,  5, 21,  7,  7, 20, 21,\n",
       "        20,  5,  7,  5,  5,  5,  7, 18, 19,  5])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7,  6, 22,  9, 20,  7, 19,  8, 10, 11, 26, 21,  9, 31,  7, 10,  9, 23,\n",
       "         6,  8,  7, 13, 10, 22,  6, 21, 23, 23, 10,  8, 31,  6, 24,  8, 22, 11,\n",
       "        13, 24, 27, 26, 23, 21,  7, 22, 20,  7, 20, 22,  8, 23, 13, 13, 26, 22,\n",
       "        21, 11, 10,  7,  7, 11, 13, 20, 23,  7])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6740, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.losses['ce'](o['s_target'], s_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_target, e_target = map(lambda x: x.squeeze(-1).contiguous(), x['target_relation'].split(1, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4811, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.losses['ce'](o['intent'], x['intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['tags'].view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 82])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o['tags'].view(-1, 82).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab7f212329a491b497f27876271d03c022f2dd26760015eef69af619991238fd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('venv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
