{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "src_path = Path('.').absolute().parent\n",
    "data_path = src_path / 'data'\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "import yaml\n",
    "import networkx as nx\n",
    "from src.ontology import OntologySystem\n",
    "\n",
    "with (src_path / 'setting_files' / 'app_settings.yml').open('r') as file:\n",
    "    settings = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "onto = OntologySystem(\n",
    "    acc_name_path=data_path / 'AccountName.csv', \n",
    "    rdf_path=data_path / 'AccountRDF.xml',\n",
    "    model_path=data_path / settings['ontology']['model']['model_name'],\n",
    "    kwargs_graph_drawer=settings['ontology']['graph_drawer']\n",
    ")\n",
    "ACC_DICT = onto.ACC_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for guessing masking tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "\n",
    "model_path = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForMaskedLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Asking information based on fact and knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "# what is the Cost of sales ratio in last year?\n",
    "threshold = 0.01\n",
    "exceptions = ['BalanceSheet', 'IncomeStatement', 'CalendarOneYear']\n",
    "times = ['year', 'quarter']\n",
    "sentence_format = \"[MASK] is the {} in the [MASK] {}?\"\n",
    "n_top = 15\n",
    "\n",
    "predicted_tokens_dict = defaultdict(set)\n",
    "progress_bar = tqdm(total=((len(ACC_DICT) - len(exceptions)) * len(times)))\n",
    "\n",
    "for acc, dic in ACC_DICT.items():\n",
    "    if acc in exceptions:\n",
    "        continue\n",
    "    account_name = dic['eng_name'].lower()\n",
    "    for t in times:\n",
    "        s = sentence_format.format(account_name, t.lower())\n",
    "\n",
    "        inputs = tokenizer(s, padding=True, truncation=True, return_token_type_ids=True, return_tensors='pt')\n",
    "        inputs_tensors = inputs['input_ids']\n",
    "        masked = inputs_tensors.eq(tokenizer.mask_token_id)\n",
    "        outputs = model(**inputs).logits[masked]\n",
    "        logits_top = outputs.argsort(descending=True)[:, :n_top]\n",
    "        probs_top = outputs.softmax(1).gather(1, logits_top)\n",
    "\n",
    "        for i, m in enumerate(probs_top >= threshold):\n",
    "            # tkns.append([k.item() for k in logits_top[i, m]])\n",
    "            for k in logits_top[i, m]:\n",
    "                tkn = tokenizer.decode(k)\n",
    "                if len(re.findall(r'(\\\")', tkn)) == 0:\n",
    "                    predicted_tokens_dict[f'[MASK]-{i}-{t}'].add(tkn)\n",
    "        \n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (data_path / 'tkns.csv').open('w') as file:\n",
    "    for k, v in predicted_tokens_dict.items():\n",
    "        print(','.join([k] + list(v)), file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "What if: Analysis based on fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge = 'BS'\n",
    "knowledge_query = onto.sparql.get_predefined_knowledge(knowledge=knowledge+'R')\n",
    "results = onto.sparql.query(knowledge_query)\n",
    "nx_graph = onto.get_nx_graph(results)\n",
    "sub_tree = nx.bfs_successors(nx_graph, source='BalanceSheet')\n",
    "sub_tree = dict(sub_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253e563a860c43589fa9ba385fae17aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question 2\n",
    "# what happens to the operating income when the cost of sales increases by 10% this year?\n",
    "\n",
    "threshold = 0.01\n",
    "exceptions = ['BalanceSheet', 'IncomeStatement', 'CalendarOneYear']\n",
    "times = ['year', 'quarter']\n",
    "# sentence_format = \"what [MASK] to the {} when the {} [MASK] by {} {} in the [MASK] {}?\"\n",
    "sentence_format = \"what will be the effect to {} if the {} [MASK] by {} {} in the [MASK] {}?\"\n",
    "n_top = 15\n",
    "successors = []\n",
    "predicted_tokens_dict = defaultdict(set)\n",
    "progress_bar = tqdm()\n",
    "\n",
    "for sub_acc, accs in sub_tree.items():\n",
    "    if sub_acc in exceptions:\n",
    "        continue\n",
    "    sub_acc_name = ACC_DICT[sub_acc]['eng_name'].lower()\n",
    "    successors.extend(accs)\n",
    "    for acc in successors:\n",
    "        account_name = ACC_DICT[acc]['eng_name'].lower()\n",
    "        for t in times:\n",
    "            s = sentence_format.format(\n",
    "                account_name, sub_acc_name, \n",
    "                np.random.randint(1, 50, (1,))[0], np.random.choice(['percent', '%']),\n",
    "                t.lower())\n",
    "\n",
    "            inputs = tokenizer(s, padding=True, truncation=True, return_token_type_ids=True, return_tensors='pt')\n",
    "            inputs_tensors = inputs['input_ids']\n",
    "            masked = inputs_tensors.eq(tokenizer.mask_token_id)\n",
    "            outputs = model(**inputs).logits[masked]\n",
    "            logits_top = outputs.argsort(descending=True)[:, :n_top]\n",
    "            probs_top = outputs.softmax(1).gather(1, logits_top)\n",
    "            for i, m in enumerate(probs_top >= threshold):\n",
    "                # tkns.append([k.item() for k in logits_top[i, m]])\n",
    "                for k in logits_top[i, m]:\n",
    "                    tkn = tokenizer.decode(k)\n",
    "                    if len(re.findall(r'(\\\")', tkn)) == 0:\n",
    "                        predicted_tokens_dict[f'[MASK]-{i}-{t}'].add(tkn)\n",
    "\n",
    "            progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (data_path / 'tkns.csv').open('w') as file:\n",
    "    for k, v in predicted_tokens_dict.items():\n",
    "        print(','.join([k] + list(v)), file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "What if: Forecasting with embedded ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5739072ae18481e96aad1149d926da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question 3\n",
    "# what will be our revenue in the 4th quarter?\n",
    "\n",
    "threshold = 0.01\n",
    "exceptions = ['BalanceSheet', 'IncomeStatement', 'CalendarOneYear']\n",
    "times = ['year', 'quarter']\n",
    "sentence_format = \"[MASK] will be the {} in the [MASK] {}?\"\n",
    "# sentence_format = \"how is the {} going to be in the [MASK] {}?\"\n",
    "n_top = 15\n",
    "\n",
    "predicted_tokens_dict = defaultdict(set)\n",
    "progress_bar = tqdm(total=((len(ACC_DICT) - len(exceptions)) * len(times)))\n",
    "\n",
    "for acc, dic in ACC_DICT.items():\n",
    "    if acc in exceptions:\n",
    "        continue\n",
    "    account_name = dic['eng_name'].lower()\n",
    "    for t in times:\n",
    "        s = sentence_format.format(account_name, t.lower())\n",
    "\n",
    "        inputs = tokenizer(s, padding=True, truncation=True, return_token_type_ids=True, return_tensors='pt')\n",
    "        inputs_tensors = inputs['input_ids']\n",
    "        masked = inputs_tensors.eq(tokenizer.mask_token_id)\n",
    "        outputs = model(**inputs).logits[masked]\n",
    "        logits_top = outputs.argsort(descending=True)[:, :n_top]\n",
    "        probs_top = outputs.softmax(1).gather(1, logits_top)\n",
    "\n",
    "        for i, m in enumerate(probs_top >= threshold):\n",
    "            # tkns.append([k.item() for k in logits_top[i, m]])\n",
    "            for k in logits_top[i, m]:\n",
    "                tkn = tokenizer.decode(k)\n",
    "                if len(re.findall(r'(\\\")', tkn)) == 0:\n",
    "                    predicted_tokens_dict[f'[MASK]-{i}-{t}'].add(tkn)\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "\n",
    "with (data_path / 'tkns.csv').open('w') as file:\n",
    "    for k, v in predicted_tokens_dict.items():\n",
    "        print(','.join([k] + list(v)), file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: position 만들기\n",
    "# (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]})\n",
    "\n",
    "def get_entity(s, x, tag):\n",
    "    idx = s.index(x)\n",
    "    return (idx, idx+len(x), tag)\n",
    "\n",
    "def random_sampling(x_dict, x_key):\n",
    "    idx_range = np.arange(len(x_dict[x_key]))\n",
    "    idx = np.random.choice(idx_range, replace=False, p=np.ones(len(idx_range)) / len(idx_range))\n",
    "    word, tag, desc = x_dict[x_key][idx]\n",
    "    return word, tag, desc\n",
    "\n",
    "def get_words_filtered(words, text):\n",
    "    words_filtered = defaultdict(list)\n",
    "    for k, v in words.items():\n",
    "        for word, tag, desc in v:\n",
    "            if desc != text:\n",
    "                words_filtered[k].append((word, tag, desc))\n",
    "    return words_filtered\n",
    "\n",
    "df = pd.read_csv(data_path / 'AccountWords.csv', encoding='utf-8')\n",
    "\n",
    "format_dict = {\n",
    "    0: ['help'],\n",
    "    1: [\n",
    "        # what/how, target_account, [MASK] + year/quarter\n",
    "        \"{} is the {} in the {} ?\",\n",
    "        # [MASK] + year/quarter, what/how, target_account\n",
    "        \"In the {}, {} is the value of the {} ?\"\n",
    "    ], \n",
    "    2: [\n",
    "        # target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "        \"what happens to the {} when the {} {} by {} in the {} ?\",\n",
    "        # target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "        \"what will be the effect to {} if the {} {} by {} in the {} ?\",\n",
    "        # reverse the relation\n",
    "        # subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "        \"when the {} {} by {} in the {}, what will happen to the {} ?\",\n",
    "        # subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "        \"if the {} {} by {} in the {}, what will be the effect to {} ?\"\n",
    "    ],\n",
    "    3: [\n",
    "        # what/how, target_account, [MASK] + year/quarter\n",
    "        \"{} will be the {} in the {} ?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# TODO: maybe add the today's information after [SEP]?\n",
    "context = ['HELP', 'PAST', 'FUTURE']\n",
    "words = defaultdict(list)\n",
    "for typ in ['year', 'quarter', 'words']:\n",
    "    df_temp = df.loc[:, [typ, f'{typ}_tag', f'{typ}_desc']]\n",
    "    df_temp = df_temp.loc[~df_temp[typ].isna(), :]\n",
    "    for i, (w, t, desc) in df_temp.iterrows():\n",
    "        words[typ].append((w, t, desc))\n",
    "\n",
    "exceptions = ['BalanceSheet', 'IncomeStatement', 'Ratios', 'CalendarOneYear']\n",
    "times = ['year', 'quarter']\n",
    "\n",
    "all_data = []\n",
    "s_ENT = '[E]'\n",
    "e_ENT = '[/E]'\n",
    "f_ENT = lambda x: f'{s_ENT}{x}{e_ENT}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "```python\n",
    "# what/how, target_account, [MASK] + year/quarter\n",
    "\"{} is the {} in the {}?\",\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062d15cb84954c2fb79e64052efda37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data1 = []\n",
    "trg_scenario = 1\n",
    "progress_bar = tqdm()\n",
    "words_filtered = get_words_filtered(words, text='FUTURE')\n",
    "for idx_fmt, fmt in enumerate(format_dict[trg_scenario]):\n",
    "    \n",
    "    for acc, dic in ACC_DICT.items():\n",
    "        if acc in exceptions:\n",
    "            continue\n",
    "        target_account = dic['eng_name'].lower()\n",
    "        knowledge, acc_type, _ = dic['group'].split('-')\n",
    "\n",
    "        for t in ['year', 'quarter']:\n",
    "            for t_word, t_tag, _ in words_filtered[t]:\n",
    "                entities = []\n",
    "                pre_token = np.random.choice(['what', 'how'], replace=False, p=np.ones(2)/2)\n",
    "                if idx_fmt == 0:\n",
    "                    # what/how, target_account, [MASK] + year/quarter\n",
    "                    # \"{} is the {} in the {}?\",\n",
    "                    s = fmt.format(\n",
    "                        pre_token,\n",
    "                        f_ENT(target_account), \n",
    "                        f_ENT(f'{t_word} {t}')\n",
    "                        )\n",
    "                else:\n",
    "                    # [MASK] + year/quarter, what/how, target_account\n",
    "                    # \"In the {}, {} is the value of the {}\"\n",
    "                    s = fmt.format(\n",
    "                        f_ENT(f'{t_word} {t}'),\n",
    "                        pre_token,\n",
    "                        f_ENT(target_account)\n",
    "                    )\n",
    "                # relation = [0, 0, 0]  # no_relation, order1, order2\n",
    "                # entities\n",
    "                ## target_account\n",
    "                entities.append(get_entity(s, f_ENT(target_account), f'{knowledge}.{acc_type}'))\n",
    "                ## MASK year/quarter\n",
    "                entities.append(get_entity(s, f_ENT(f'{t_word} {t}'), t_tag))\n",
    "                \n",
    "                data1.append(\n",
    "                    {'question': s, 'entities': sorted(entities, key=lambda x: x[0]), 'intent': 'PAST.value'} #, 'relation': relation}\n",
    "                )\n",
    "            \n",
    "                progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "```python\n",
    "# target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "\"what happens to the {} when the {} {} by {} in the {}?\"\n",
    "# target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "\"what will be the effect to {} if the {} {} by {} in the {}?\"\n",
    "# reverse the relation\n",
    "# subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "\"when the {} {} by {} in the {}, what will happen to the {}?\"\n",
    "# subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "\"if the {} {} by {} in the {}, what will be the effect to {}?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_role_dict(onto, knowledge):\n",
    "    knowledge_query = onto.sparql.get_predefined_knowledge(knowledge=knowledge)\n",
    "    sparql_results = onto.sparql.query(knowledge_query)\n",
    "    role_dict = defaultdict(list)\n",
    "    for s, p, o in sparql_results:\n",
    "        s, p, o = map(onto.graph_drawer.convert_to_string, [s, p, o])\n",
    "        if s == 'CalendarOneYear' or o == 'CalendarOneYear':\n",
    "            continue\n",
    "        if s not in role_dict[o]:\n",
    "            role_dict[o].append(s)\n",
    "        \n",
    "    return role_dict\n",
    "\n",
    "def process_successor(successors, role_dict, trg_acc, acc):\n",
    "    if role_dict.get(acc) is None:\n",
    "        # successors[trg_acc].extend(successors[acc])\n",
    "        return None\n",
    "    else:\n",
    "        accs = role_dict.get(acc)\n",
    "        if accs is not None:\n",
    "            successors[trg_acc].extend(accs)\n",
    "            for acc in accs:\n",
    "                process_successor(successors, role_dict, trg_acc, acc)\n",
    "\n",
    "def get_successor(onto, knowledge, exceptions=None):\n",
    "    role_dict = get_role_dict(onto, knowledge=knowledge)\n",
    "    successors = defaultdict(list)\n",
    "    for trg_acc in role_dict.keys():\n",
    "        if (exceptions is not None) and (trg_acc in exceptions):\n",
    "            continue\n",
    "        process_successor(successors, role_dict, trg_acc, trg_acc)\n",
    "    return successors\n",
    "\n",
    "trg_scenario = 2\n",
    "bs_successors = get_successor(sparql, 'BS', exceptions)\n",
    "is_successors = get_successor(sparql, 'IS', exceptions)\n",
    "data2 = []\n",
    "n_sample = 5\n",
    "progress_bar = tqdm()\n",
    "words_filtered = get_words_filtered(words, text='FUTURE')\n",
    "\n",
    "for idx_fmt, fmt in enumerate(format_dict[trg_scenario]):\n",
    "    for sub_tree in [bs_successors, is_successors]:\n",
    "        for trg_acc, successors in sub_tree.items():\n",
    "            if trg_acc in exceptions:\n",
    "                continue\n",
    "            target_account = ACC_DICT[trg_acc]['eng_name'].lower()\n",
    "            target_knowledge, target_acc_type, _ = ACC_DICT[trg_acc]['group'].split('-')\n",
    "            for sub_acc in successors:\n",
    "                subject_account = ACC_DICT[sub_acc]['eng_name'].lower()\n",
    "                subject_knowledge, subject_acc_type, _ = ACC_DICT[trg_acc]['group'].split('-')\n",
    "                n = 0\n",
    "                while n < n_sample:\n",
    "                    entities = []\n",
    "\n",
    "                    apply_word, apply_tag, _ = random_sampling(x_dict=words_filtered, x_key='words')\n",
    "                    t = np.random.choice(times, replace=False, p=np.ones(len(times))/len(times))\n",
    "                    t_word, t_tag, _ = random_sampling(x_dict=words_filtered, x_key=t)\n",
    "                    \n",
    "                    number = np.random.randint(1, 99)\n",
    "                    percent = np.random.choice(['percent', '%'], replace=False, p=np.ones(2)/2)\n",
    "                    \n",
    "                    if idx_fmt in [0, 1]:\n",
    "                        # target_account, subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter\n",
    "                        s = fmt.format(\n",
    "                            f_ENT(target_account),\n",
    "                            f_ENT(subject_account), \n",
    "                            f_ENT(apply_word), \n",
    "                            f_ENT(f'{number} {percent}'),\n",
    "                            f_ENT(f'{t_word} {t}')\n",
    "                            )\n",
    "                        # relation = [1, 1, 2]\n",
    "                    else:\n",
    "                        # subject_account, [MASK], random_number + percent/%, [MASK] + year/quarter, target_account\n",
    "                        s = fmt.format(\n",
    "                            f_ENT(subject_account), \n",
    "                            f_ENT(apply_word), \n",
    "                            f_ENT(f'{number} {percent}'),\n",
    "                            f_ENT(f'{t_word} {t}'),\n",
    "                            f_ENT(target_account)\n",
    "                            )\n",
    "                        # relation = [1, 2, 1]\n",
    "                    # entities\n",
    "                    ## target_account\n",
    "                    entities.append(get_entity(s, f_ENT(target_account), f'{target_knowledge}.{target_acc_type}'))\n",
    "                    ## subject_account\n",
    "                    entities.append(get_entity(s, f_ENT(subject_account), f'{subject_knowledge}.{subject_acc_type}'))\n",
    "                    ## MASK apply words\n",
    "                    entities.append(get_entity(s, f_ENT(apply_word), apply_tag))\n",
    "                    ## percentages\n",
    "                    entities.append(get_entity(s, f_ENT(f'{number} {percent}'), 'PERCENT'))\n",
    "                    ## MASK year/quarter\n",
    "                    entities.append(get_entity(s, f_ENT(f'{t_word} {t}'), t_tag))\n",
    "\n",
    "                    d = {'question': s, 'entities': sorted(entities, key=lambda x: x[0]), 'intent': 'IF.fact'} #, 'relation': relation}\n",
    "                    if d not in data2:\n",
    "                        data2.append(\n",
    "                            d\n",
    "                        )\n",
    "                    \n",
    "                    progress_bar.update(1)\n",
    "                    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "```python\n",
    "# what/how, target_account, [MASK] + year/quarter\n",
    "\"{} will be the {} in the {}?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119fd2afaac74f688b65fd2e1c464fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1014\n"
     ]
    }
   ],
   "source": [
    "data3 = []\n",
    "trg_scenario = 3\n",
    "progress_bar = tqdm()\n",
    "words_filtered = get_words_filtered(words, text='PAST')\n",
    "\n",
    "for fmt in format_dict[trg_scenario]:\n",
    "    for acc, dic in ACC_DICT.items():\n",
    "        if acc in exceptions:\n",
    "            continue\n",
    "        target_account = dic['eng_name'].lower()\n",
    "        knowledge, acc_type, _ = dic['group'].split('-')\n",
    "        for t in ['year', 'quarter']:\n",
    "            for t_word, t_tag, _ in words_filtered[t]:\n",
    "                entities = []\n",
    "                s = fmt.format(\n",
    "                    np.random.choice(['what', 'how']), \n",
    "                    f_ENT(target_account), \n",
    "                    f_ENT(f'{t_word} {t}')\n",
    "                    )\n",
    "                # relation = [0, 0, 0]\n",
    "                # entities\n",
    "                ## target_account\n",
    "                entities.append(get_entity(s, f_ENT(target_account), f'{knowledge}.{acc_type}'))\n",
    "                ## MASK year/quarter\n",
    "                entities.append(get_entity(s, f_ENT(f'{t_word} {t}'), t_tag))\n",
    "                \n",
    "                data3.append(\n",
    "                    {'question': s, 'entities': entities, 'intent': 'IF.forecast'} #, 'relation': relation}\n",
    "                )\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "\n",
    "all_data = data1 + data2 + data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process for entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "special_len = len(s_ENT)+len(e_ENT)\n",
    "\n",
    "for k, x in tqdm(enumerate(all_data), total=len(all_data)):\n",
    "    all_data[k]['question'] = x['question'].replace(s_ENT, '').replace(e_ENT, '')\n",
    "    for i, (s, e, ent) in enumerate(x['entities']):\n",
    "        new_s = s-i*special_len\n",
    "        new_e = new_s+(e-s)-special_len\n",
    "        all_data[k]['entities'][i] = (new_s, new_e, ent)\n",
    "\n",
    "with (data_path / 'all_data.jsonl').open('w', encoding='utf-8') as file:\n",
    "    for line in tqdm(all_data, total=len(all_data), desc='saving'):\n",
    "        file.write(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Debugging]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with (data_path / 'labels.json').open('r', encoding='utf-8') as file:\n",
    "    labels = json.load(file)\n",
    "\n",
    "tags2id = {v: k for k, v in enumerate(labels['tags'])}\n",
    "intents2id = {v: k for k, v in enumerate(labels['intent'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2314\n",
    "data = train_data[index]\n",
    "text = data['text']\n",
    "ents = data['entities']\n",
    "intent = data['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlu_tokenizer.tokenize(text)\n",
    "offset_mapping = nlu_tokenizer.str_to_offset_mapping(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'the', 'advances', 'customers', 'rise', 'by', '94', 'percent', 'in', 'the', 'calendar', 'year', ',', 'what', 'will', 'happen', 'to', 'the', 'lia', '##bilities', 'and', 'e', '##qui', '##ties', '?']\n",
      "['O', 'O', 'B-BS.Value', 'I-BS.Value', 'B-APPLY', 'O', 'B-PERCENT', 'I-PERCENT', 'O', 'O', 'B-TIME', 'I-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'B-BS.Value', 'I-BS.Value', 'I-BS.Value', 'I-BS.Value', 'I-BS.Value', 'I-BS.Value', 'O']\n"
     ]
    }
   ],
   "source": [
    "tags = nlu_tokenizer.get_biluo_tags(tokens, offset_mapping, ents)\n",
    "tags = biluo_to_iob(tags)\n",
    "print(tokens)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = nlu_tokenizer(\n",
    "    text, \n",
    "    add_special_tokens=True, \n",
    "    padding='max_length', \n",
    "    truncation=True, \n",
    "    max_length=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tags(tags, input_ids, pad_idx):\n",
    "    padded_tags = [pad_idx] + tags + [pad_idx] + ([pad_idx] * (len(input_ids) - len(tags)))\n",
    "    return padded_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlu_tokenizer.tokenize(text)\n",
    "offset_mapping = nlu_tokenizer.spacy_encode(text, pad_offset=False)['offset_mapping']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine conll dataset and custom dataset into together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizerFast, BertConfig\n",
    "\n",
    "model_path = 'bert-base-uncased'# 'dslim/bert-base-NER'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "bert_ner = BertForTokenClassification.from_pretrained(model_path)\n",
    "# cfg = BertConfig.from_pretrained(model_path, label2id=, id2label=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning\n",
    "\n",
    "- Entities\n",
    "- Entities Relation (subject, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading: 100%|██████████| 18129/18129 [00:00<00:00, 87565.78it/s]\n",
      "loading: 100%|██████████| 3705/3705 [00:00<00:00, 161522.27it/s]\n",
      "loading: 100%|██████████| 3693/3693 [00:00<00:00, 139501.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.nlu_utils import NLUDataModule, NLUTokenizer, NLUDataset\n",
    "\n",
    "main_path = Path().absolute().parent\n",
    "data_path = main_path / 'data'\n",
    "setting_path = main_path / 'setting_files'\n",
    "\n",
    "with (setting_path / 'train_settings.yml').open('r') as file:\n",
    "    settings = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "nlu_tokenizer = NLUTokenizer()\n",
    "\n",
    "data_module = NLUDataModule(\n",
    "    train_path=data_path / settings['train_file'], \n",
    "    valid_path=data_path / settings['valid_file'],\n",
    "    test_path=data_path / settings['test_file'],\n",
    "    labels_path=data_path / settings['labels_file'],\n",
    "    batch_size=settings['batch_size'], \n",
    "    max_len=settings['max_len'],\n",
    "    num_workers=settings['num_workers'],\n",
    "    seed=settings['seed']\n",
    ")\n",
    "data_module.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_module.train_data\n",
    "train_dataset = NLUDataset(\n",
    "    train_data, \n",
    "    tags2id=data_module.tags2id, \n",
    "    intents2id=data_module.intents2id,\n",
    ")\n",
    "train_dataloader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what happens to the sales and selling general administrative ratio when the cost of sales ratio decreases by 32 % in the past year?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from spacy.training import biluo_tags_to_offsets, iob_to_biluo, offsets_to_biluo_tags\n",
    "\n",
    "idx = 1468\n",
    "text = train_data[idx]['text']\n",
    "ents = train_data[idx]['entities']\n",
    "intent = train_data[idx]['intent']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillmask = pipeline('fill-mask', model=\"distilroberta-base\")\n",
    "mask_token = fillmask.tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = nlu_tokenizer.get_tags(text, ents, tag_type='iob')\n",
    "spacy_tokens = nlu_tokenizer.spacy_tokenize(text)\n",
    "bert_tokens = nlu_tokenizer.bert_tokenize(text)\n",
    "spanned_tags = nlu_tokenizer.map_spanned_tokens(\n",
    "    longer_tokens=bert_tokens, shorter_token=spacy_tokens, tags=tags\n",
    ")\n",
    "bert_encodes = nlu_tokenizer(\n",
    "    text, \n",
    "    add_special_tokens=True, \n",
    "    truncation=True, \n",
    "    max_length=256\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-IS.Ratio',\n",
       " 'I-IS.Ratio',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-IS.Ratio',\n",
       " 'I-IS.Ratio',\n",
       " 'I-IS.Ratio',\n",
       " 'B-APPLY',\n",
       " 'O',\n",
       " 'B-PERCENT',\n",
       " 'I-PERCENT',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-TIME',\n",
       " 'I-TIME',\n",
       " 'O']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = []\n",
    "for sentence in spanned_tags:\n",
    "    words = sentence.split(' ')\n",
    "    K = np.random.randint(1, len(words)-1)\n",
    "    masked_sentence = \" \".join(words[:K]  + [mask_token] + words[K+1:])\n",
    "    predictions = fillmask(masked_sentence)\n",
    "    augmented_sequences = [predictions[i]['sequence'] for i in range(3)]\n",
    "    outputs += [sentence] + augmented_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thrun|PERSON\n",
      "Google|ORG\n",
      "2007|DATE\n",
      "American|GPE\n",
      "Thrun|PERSON\n",
      "Recode|ORG\n",
      "earlier this week|DATE\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"When Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously. \"I can tell you very senior CEOs of major American car companies would shake my hand and turn away because I was't worth talking to,\" said Thrun, in an interview with Recode earlier this week.\"\"\"\n",
    "entities = [\n",
    "    (5, 10, 'PERSON'), (51, 57, 'ORG'), (61, 65, 'DATE'), (163, 171, 'GPE'), \n",
    "    (260, 265, 'PERSON'), (288, 294, 'ORG'), (295, 312, 'DATE')\n",
    "]\n",
    "for s, e, t in entities:\n",
    "    print(f'{text[s:e]}|{t}')\n",
    "\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_offset_mapping = nlu_tokenizer.bert(text, add_special_tokens=False, return_offsets_mapping=True)['offset_mapping']\n",
    "bert_tokens = nlu_tokenizer.bert_tokenize(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-PERSON',\n",
       " 'L-PERSON',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'U-ORG',\n",
       " 'O',\n",
       " 'U-DATE',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'U-GPE',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PERSON',\n",
       " 'L-PERSON',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'L-ORG',\n",
       " 'B-DATE',\n",
       " 'I-DATE',\n",
       " 'L-DATE',\n",
       " 'O']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset_mapping = bert_offset_mapping\n",
    "ents = entities\n",
    "tokens = bert_tokens\n",
    "\n",
    "starts, ends = dict(), dict()\n",
    "for tkn_idx, (s_idx, e_idx) in enumerate(offset_mapping):\n",
    "    if s_idx == e_idx == 0:\n",
    "        continue\n",
    "    starts[s_idx] = tkn_idx\n",
    "    ends[e_idx] = tkn_idx\n",
    "\n",
    "char_in_ents = {}\n",
    "labels = ['-'] * len(offset_mapping)\n",
    "for s_char, e_char, ent in ents:\n",
    "    if not ent:\n",
    "        for s in starts:  # account for many-to-one\n",
    "            if s >= s_char and s < e_char:\n",
    "                labels[starts[s]] = 'O'\n",
    "    else:\n",
    "        for char_idx in range(s_char, e_char):\n",
    "            if char_idx in char_in_ents.keys():\n",
    "                raise ValueError(f'Trying to Overlapping same tokens: {char_in_ents[char_idx]} / {(s_char, e_char, ent)}')\n",
    "            char_in_ents[char_idx] = (s_char, e_char, ent)\n",
    "        s_token = starts.get(s_char)\n",
    "        e_token = ends.get(e_char)\n",
    "\n",
    "        if s_token is not None and e_token is not None:\n",
    "            if s_token == e_token:\n",
    "                labels[s_token] = f\"U-{ent}\"\n",
    "            else:\n",
    "                labels[s_token] = f\"B-{ent}\"\n",
    "                for i in range(s_token + 1, e_token):\n",
    "                    labels[i] = f\"I-{ent}\"\n",
    "                labels[e_token] = f\"L-{ent}\"\n",
    "                \n",
    "entity_chars = set()\n",
    "for s_char, e_char, ent in ents:\n",
    "    for i in range(s_char, e_char):\n",
    "        entity_chars.add(i)\n",
    "for token_idx, (s, e) in enumerate(offset_mapping):\n",
    "    for i in range(s, e):\n",
    "        if i in entity_chars:\n",
    "            break\n",
    "    else:\n",
    "        labels[token_idx] = 'O'\n",
    "if '-' in labels:\n",
    "    print(labels.index('-'))\n",
    "    raise ValueError('Some Tokens are not properly assigned' + f'{labels}')\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: (5, 10, 'PERSON'),\n",
       " 6: (5, 10, 'PERSON'),\n",
       " 7: (5, 10, 'PERSON'),\n",
       " 8: (5, 10, 'PERSON'),\n",
       " 9: (5, 10, 'PERSON'),\n",
       " 51: (51, 57, 'ORG'),\n",
       " 52: (51, 57, 'ORG'),\n",
       " 53: (51, 57, 'ORG'),\n",
       " 54: (51, 57, 'ORG'),\n",
       " 55: (51, 57, 'ORG'),\n",
       " 56: (51, 57, 'ORG'),\n",
       " 61: (61, 65, 'DATE'),\n",
       " 62: (61, 65, 'DATE'),\n",
       " 63: (61, 65, 'DATE'),\n",
       " 64: (61, 65, 'DATE'),\n",
       " 163: (163, 172, 'GPE'),\n",
       " 164: (163, 172, 'GPE'),\n",
       " 165: (163, 172, 'GPE'),\n",
       " 166: (163, 172, 'GPE'),\n",
       " 167: (163, 172, 'GPE'),\n",
       " 168: (163, 172, 'GPE'),\n",
       " 169: (163, 172, 'GPE'),\n",
       " 170: (163, 172, 'GPE'),\n",
       " 171: (163, 172, 'GPE'),\n",
       " 260: (260, 265, 'PERSON'),\n",
       " 261: (260, 265, 'PERSON'),\n",
       " 262: (260, 265, 'PERSON'),\n",
       " 263: (260, 265, 'PERSON'),\n",
       " 264: (260, 265, 'PERSON'),\n",
       " 288: (288, 294, 'ORG'),\n",
       " 289: (288, 294, 'ORG'),\n",
       " 290: (288, 294, 'ORG'),\n",
       " 291: (288, 294, 'ORG'),\n",
       " 292: (288, 294, 'ORG'),\n",
       " 293: (288, 294, 'ORG'),\n",
       " 295: (295, 312, 'DATE'),\n",
       " 296: (295, 312, 'DATE'),\n",
       " 297: (295, 312, 'DATE'),\n",
       " 298: (295, 312, 'DATE'),\n",
       " 299: (295, 312, 'DATE'),\n",
       " 300: (295, 312, 'DATE'),\n",
       " 301: (295, 312, 'DATE'),\n",
       " 302: (295, 312, 'DATE'),\n",
       " 303: (295, 312, 'DATE'),\n",
       " 304: (295, 312, 'DATE'),\n",
       " 305: (295, 312, 'DATE'),\n",
       " 306: (295, 312, 'DATE'),\n",
       " 307: (295, 312, 'DATE'),\n",
       " 308: (295, 312, 'DATE'),\n",
       " 309: (295, 312, 'DATE'),\n",
       " 310: (295, 312, 'DATE'),\n",
       " 311: (295, 312, 'DATE')}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_in_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens = nlu_tokenizer.spacy_tokenize(text)\n",
    "bert_encodes = nlu_tokenizer.bert(text, add_special_tokens=False, return_offsets_mapping=True)\n",
    "bert_offset_mapping = bert_encodes['offset_mapping']\n",
    "# bert_tokens = [nlu_tokenizer.bert_decode([i]) for i in bert_encodes['input_ids']]\n",
    "bert_tokens = nlu_tokenizer.bert_tokenize(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When__Thrun__started__working__on__self__-__driving__cars__at__Google__in__2007__,__few__people__outside__of__the__company__took__him__seriously__.__\"__I__can__tell__you__very__senior__CEOs__of__major__American__car__companies__would__shake__my__hand__and__turn__away__because__I__was__n\\'t__worth__talking__to__,__\"__said__Thrun__,__in__an__interview__with__Recode__earlier__this__week__.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'__'.join(spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when__thrun__started__working__on__self-driving__cars__at__google__in__2007,__few__people__outside__of__the__company__took__him__seriously.__\"i__can__tell__you__very__senior__ceos__of__major__american__car__companies__would__shake__my__hand__and__turn__away__because__i__wasn\\'t__worth__talking__to,\"__said__thrun,__in__an__interview__with__recode__earlier__this__week.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_txt = ''\n",
    "for i, t in enumerate(bert_tokens):\n",
    "    s_cur, e_cur = bert_offset_mapping[i]\n",
    "    s_prev, e_prev = bert_offset_mapping[i - 1] if i != 0 else (-1, 0)\n",
    "    if e_prev == s_cur:\n",
    "        bert_txt += ''\n",
    "    else:\n",
    "        bert_txt += '__'\n",
    "\n",
    "    if t.startswith('##'):\n",
    "        bert_txt += t[2:]\n",
    "    else:\n",
    "        bert_txt += t\n",
    "\n",
    "bert_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "\n",
    "s_cur, e_cur = bert_offset_mapping[i]\n",
    "s_prev, e_prev = bert_offset_mapping[i - 1] if i != 0 else (-1, -1)\n",
    "a, b = bert_offset_mapping[i - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_txt == '__'.join(spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_offset_mapping[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'when',\n",
       " 'thru',\n",
       " '##n',\n",
       " 'started',\n",
       " 'working',\n",
       " 'on',\n",
       " 'self',\n",
       " '-',\n",
       " 'driving',\n",
       " 'cars',\n",
       " 'at',\n",
       " 'google',\n",
       " 'in',\n",
       " '2007',\n",
       " ',',\n",
       " 'few',\n",
       " 'people',\n",
       " 'outside',\n",
       " 'of',\n",
       " 'the',\n",
       " 'company',\n",
       " 'took',\n",
       " 'him',\n",
       " '##ser',\n",
       " '##iously',\n",
       " '.',\n",
       " '\"',\n",
       " 'i',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'very',\n",
       " 'senior',\n",
       " 'ceo',\n",
       " '##s',\n",
       " 'of',\n",
       " 'major',\n",
       " 'american',\n",
       " '##car',\n",
       " 'companies',\n",
       " 'would',\n",
       " 'shake',\n",
       " 'my',\n",
       " 'hand',\n",
       " 'and',\n",
       " 'turn',\n",
       " 'away',\n",
       " 'because',\n",
       " 'i',\n",
       " 'wasn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'worth',\n",
       " 'talking',\n",
       " 'to',\n",
       " ',',\n",
       " '\"',\n",
       " 'said',\n",
       " 'thru',\n",
       " '##n',\n",
       " ',',\n",
       " 'in',\n",
       " 'an',\n",
       " 'interview',\n",
       " 'with',\n",
       " 'rec',\n",
       " '##ode',\n",
       " 'earlier',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nlu_tokenizer.bert_decode([i]) for i in nlu_tokenizer(text)['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['was', \"n't\"], ['wasn', \"'\", 't'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokens, bert_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2314\n",
    "data = train_data[index]\n",
    "text = data['text']\n",
    "ents = data['entities']\n",
    "intent = data['intent']\n",
    "\n",
    "encodes = nlu_tokenizer.bert.encode_plus(\n",
    "    text, \n",
    "    add_special_tokens=True, \n",
    "    truncation=True, \n",
    "    max_length=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  2129,\n",
       "  2003,\n",
       "  1996,\n",
       "  2783,\n",
       "  7045,\n",
       "  1999,\n",
       "  1996,\n",
       "  2034,\n",
       "  2095,\n",
       "  1029,\n",
       "  102],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'intent': 3,\n",
       " 'tags': [0, 0, 0, 0, 3, 4, 0, 0, 13, 14, 0, 0]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader validation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f607c65d3c164c69a1e1ff46cfaf65a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = data_module.create_dataset(data_module.train_data)\n",
    "\n",
    "restart_idx = 0\n",
    "for i in tqdm(range(restart_idx, len(train_dataset)), total=len(train_dataset)-restart_idx):\n",
    "    item = train_dataset.__getitem__(i)\n",
    "    assert item['tags'].size(0) == 256, f\"tags_size={item['tags'].size()}\"\n",
    "    assert isinstance(item['intent'].tolist(), int), f\"intent_size={item['intent']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260aaf6ef8124981a4d58a38b467eab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_dataset = data_module.create_dataset(data_module.valid_data)\n",
    "restart_idx = 0\n",
    "for i in tqdm(range(restart_idx, len(valid_dataset)), total=len(valid_dataset)-restart_idx):\n",
    "    valid_dataset.__getitem__(i)\n",
    "    assert item['tags'].size(0) == 256, f\"tags_size={item['tags'].size()}\"\n",
    "    assert isinstance(item['intent'].tolist(), int), f\"intent_size={item['intent']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d7d321221443119b85f8baaf1ebfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3693 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = data_module.create_dataset(data_module.test_data)\n",
    "restart_idx = 0\n",
    "for i in tqdm(range(restart_idx, len(test_dataset)), total=len(test_dataset)-restart_idx):\n",
    "    test_dataset.__getitem__(i)\n",
    "    assert item['tags'].size(0) == 256, f\"tags_size={item['tags'].size()}\"\n",
    "    assert isinstance(item['intent'].tolist(), int), f\"intent_size={item['intent']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data_module.train_dataloader():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Debugging]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training import biluo_to_iob, offsets_to_biluo_tags, biluo_tags_to_offsets, iob_to_biluo\n",
    "from src.nlu_utils import NLUTokenizer\n",
    "\n",
    "nlu_tokenizer = NLUTokenizer(hugg_path='bert-base-uncased', spacy_path='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11. johnny herbert ( britain ) sauber 1:56.318\n",
      "[[4, 18, 'PER'], [21, 28, 'LOC'], [31, 37, 'ORG']]\n",
      "None\n",
      "johnny herbert PER\n",
      "britain LOC\n",
      "sauber ORG\n"
     ]
    }
   ],
   "source": [
    "i = 5972 \n",
    "x = train_dataset.data[i]\n",
    "text = x['text']\n",
    "ents = x['entities']\n",
    "intent = x['intent']\n",
    "print(text)\n",
    "print(ents)\n",
    "print(intent)\n",
    "\n",
    "for x in ents:\n",
    "    print(text[x[0]:x[1]], x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O', 'B-ORG', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11', '.', 'johnny', 'herbert', '(', 'britain', ')', 'sauber', '1:56.318']\n",
      "['O', 'O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O', 'B-ORG', 'O']\n",
      "['11', '.', 'johnny', 'herbert', '(', 'britain', ')', 'sa', '##uber', '1', ':', '56', '.', '318']\n"
     ]
    }
   ],
   "source": [
    "spacy_tokens = nlu_tokenizer.spacy_tokenize(text)\n",
    "tags = nlu_tokenizer.get_tags(text, ents)\n",
    "bert_tokens = nlu_tokenizer.bert_tokenize(text)\n",
    "\n",
    "print(spacy_tokens)\n",
    "print(tags)\n",
    "print(bert_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_mappings = nlu_tokenizer.get_token_mappings(bert_tokens, spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [0],\n",
       "             1: [1],\n",
       "             2: [2],\n",
       "             3: [3],\n",
       "             4: [4],\n",
       "             5: [5],\n",
       "             6: [6],\n",
       "             7: [7, 8],\n",
       "             8: [9, 10, 11, 12, 13]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (C:\\Users\\simon\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n",
      "100%|██████████| 3/3 [00:00<00:00, 429.73it/s]\n"
     ]
    }
   ],
   "source": [
    "conll = load_dataset('conll2003')\n",
    "dataset = conll['train']\n",
    "feature = dataset.features['ner_tags'].feature\n",
    "\n",
    "for d in tqdm(dataset, total=len(dataset)):\n",
    "    if ' '.join(d['tokens']).lower() == text:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O', 'B-ORG', 'O']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = list(map(feature.int2str, d['ner_tags']))\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 10, 'PER'), (19, 20, 'LOC'), (29, 30, 'ORG')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biluo_tags_to_offsets(nlu_tokenizer.spacy_nlp(text), iob_to_biluo(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11', '.', 'johnny', 'herbert', '(', 'britain', ')', 'sauber', '1:56.318']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokens = list(map(str, nlu_tokenizer.spacy_nlp(text)))\n",
    "original_tokens = d['tokens']\n",
    "\n",
    "spacy_tokens, original_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11.', 'Johnny', 'Herbert', '(', 'Britain', ')', 'Sauber', '1:56.318']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O', 'B-ORG', 'O']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [0, 1],\n",
       "             1: [2],\n",
       "             2: [3],\n",
       "             3: [4],\n",
       "             4: [5],\n",
       "             5: [6],\n",
       "             6: [7],\n",
       "             7: [8]})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i, j = 0, 0\n",
    "token_mappings = defaultdict(list) #{spacy_idx: [bert_idx]}\n",
    "spanned = ''\n",
    "while i < len(original_tokens) and j < len(spacy_tokens):\n",
    "    origin_tkn = original_tokens[i]\n",
    "    spacy_tkn = spacy_tokens[j]\n",
    "    if origin_tkn == spacy_tkn:\n",
    "        token_mappings[i].append(j)\n",
    "        i += 1\n",
    "        j += 1\n",
    "        spanned = ''\n",
    "    else:\n",
    "        token_mappings[i].append(j)\n",
    "        j += 1\n",
    "        spanned += spacy_tkn[2:] if spacy_tkn.startswith('##') else spacy_tkn\n",
    "        # see whether spanned is equal to current tokens\n",
    "        if len(spanned) == len(origin_tkn):\n",
    "            i += 1 \n",
    "            spanned = ''\n",
    "token_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'O', 'B-ORG', 'O']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_tags = ['-'] * len(spacy_tokens)\n",
    "for i, t in enumerate(tags):\n",
    "    for k in token_mappings[i]:\n",
    "        fixed_tags[k] = t\n",
    "fixed_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11. johnny herbert ( britain ) sauber 1:56.318"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlu_tokenizer.spacy_nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 18, 'PER'), (21, 28, 'LOC'), (31, 37, 'ORG')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = biluo_tags_to_offsets(doc, iob_to_biluo(fixed_tags))\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'britain'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[21:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shrs', 'outstanding', 'after', 'ipo', '16,668,560']\n",
      "['sh', '##rs', 'outstanding', 'after', 'ip', '##o', '16', ',', '66', '##8', ',', '560']\n",
      "5 12\n"
     ]
    }
   ],
   "source": [
    "tags = nlu_tokenizer.get_tags(text, ents, tag_type='iob')\n",
    "spacy_tokens = nlu_tokenizer.spacy_tokenize(text)\n",
    "bert_tokens = nlu_tokenizer.bert_tokenize(text)\n",
    "print(spacy_tokens)\n",
    "print(bert_tokens)\n",
    "print(len(spacy_tokens), len(bert_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (C:\\Users\\simon\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n",
      "100%|██████████| 3/3 [00:00<00:00, 376.01it/s]\n"
     ]
    }
   ],
   "source": [
    "conll = load_dataset('conll2003')\n",
    "typ = 'train'\n",
    "\n",
    "def add_conll_data(conll, nlu_tokenizer, data_list, typ='train', delete_errors=False):\n",
    "    dataset = conll[typ]\n",
    "    feature = dataset.features['ner_tags'].feature\n",
    "    errors = 0\n",
    "    for x in tqdm(dataset, total=len(dataset), desc=f'{typ}set'):\n",
    "        text = ' '.join(x['tokens']).lower()\n",
    "        doc = nlu_tokenizer.spacy_nlp(text)\n",
    "        if delete_errors and len(list(doc)) != len(x['tokens']):\n",
    "            errors += 1\n",
    "            continue\n",
    "            \n",
    "        tags = list(map(feature.int2str, x['ner_tags']))\n",
    "        if len(list(doc)) != len(x['tokens']):\n",
    "            spacy_tokens = list(map(str, doc))\n",
    "            original_tokens = x['tokens']\n",
    "            tags = nlu_tokenizer.map_spanned_tokens(\n",
    "                longer_tokens=spacy_tokens, shorter_token=original_tokens, tags=tags\n",
    "            )\n",
    "        entities = biluo_tags_to_offsets(doc, iob_to_biluo(tags))\n",
    "\n",
    "        d = {'text': text, 'entities': entities, 'intent': 'None'}\n",
    "        data_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spanned_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10640/1344002883.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspanned_tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spanned_tags' is not defined"
     ]
    }
   ],
   "source": [
    "spanned_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokens[1] == bert_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 0: shrs | 0: sh | sh\n",
      "| 0: shrs | 1: ##rs | shrs\n",
      "| 1: outstanding | 2: outstanding |\n",
      "| 2: after | 3: after |\n",
      "| 3: ipo | 4: ip | ip\n",
      "| 3: ipo | 5: ##o | ipo\n",
      "| 4: 16,668,560 | 6: 16 | 16\n",
      "| 4: 16,668,560 | 7: , | 16,\n",
      "| 4: 16,668,560 | 8: 66 | 16,66\n",
      "| 4: 16,668,560 | 9: ##8 | 16,668\n",
      "| 4: 16,668,560 | 10: , | 16,668,\n",
      "| 4: 16,668,560 | 11: 560 | 16,668,560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [0, 1], 1: [2], 2: [3], 3: [4, 5], 4: [6, 7, 8, 9, 10, 11]})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i, j = 0, 0\n",
    "token_mappings = defaultdict(list) #{spacy_idx: [bert_idx]}\n",
    "spanned = ''\n",
    "while i < len(spacy_tokens) and j < len(bert_tokens):\n",
    "    spacy_tkn = spacy_tokens[i]\n",
    "    bert_tkn = bert_tokens[j]\n",
    "    print(f'| {i}: {spacy_tkn} | {j}: {bert_tkn} |', end='')\n",
    "    if spacy_tkn == bert_tkn:\n",
    "        token_mappings[i].append(j)\n",
    "        i += 1\n",
    "        j += 1\n",
    "        spanned = ''\n",
    "        print()\n",
    "    else:\n",
    "        token_mappings[i].append(j)\n",
    "        j += 1\n",
    "        spanned += bert_tkn[2:] if bert_tkn.startswith('##') else bert_tkn\n",
    "        print(f' {spanned}')\n",
    "        # see whether spanned is equal to current tokens\n",
    "        if len(spanned) == len(spacy_tkn):\n",
    "            i += 1 \n",
    "            spanned = ''\n",
    "token_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanned_tags = ['-'] * len(bert_tokens)\n",
    "for i, t in enumerate(tags):\n",
    "    for k in token_mappings[i]:\n",
    "        spanned_tags[k] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-BS.Value',\n",
       " 'B-BS.Value',\n",
       " 'B-BS.Value',\n",
       " 'B-BS.Value',\n",
       " 'I-BS.Value',\n",
       " 'I-BS.Value',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-BS.Value',\n",
       " 'I-BS.Value',\n",
       " 'B-APPLY',\n",
       " 'O',\n",
       " 'B-PERCENT',\n",
       " 'I-PERCENT',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-TIME',\n",
       " 'I-TIME',\n",
       " 'O']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanned_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "\n",
    "bert_encodes = nlu_tokenizer.bert.encode_plus(text, return_offsets_mapping=False, return_tensors='pt')\n",
    "model_path = 'bert-base-uncased'\n",
    "model = BertForTokenClassification.from_pretrained(model_path)\n",
    "o = model(**bert_encodes)\n",
    "o.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_spanned_tags(bert_offset_mapping, spacy_offset_mapping, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "from collections import defaultdict\n",
    "from transformers import BertConfig, BertForTokenClassification\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"from https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert/modeling_bert.py#L627\"\"\"\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class NLUModel(pl.LightningModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        # self.hparams: model_path, intent_size, tags_size, max_len\n",
    "        self.outputs_keys = ['tags', 'intent']\n",
    "        # Networks\n",
    "        cfg = BertConfig()\n",
    "        self.bert_ner = BertForTokenClassification.from_pretrained(self.hparams.model_path, num_labels=self.hparams.tags_size)\n",
    "        self.bert_pooler = BertPooler(cfg)\n",
    "        self.intent_network = nn.Linear(cfg.hidden_size, self.hparams.intent_size)\n",
    "        \n",
    "        # losses\n",
    "        if self.hparams.stage == 'train':\n",
    "            self.loss = nn.CrossEntropyLoss()\n",
    "            # metrics\n",
    "            self.metrics = nn.ModuleDict({\n",
    "                'train_': self.create_metrics(prefix='train_'),\n",
    "                'val_': self.create_metrics(prefix='val_'),\n",
    "                'test_': self.create_metrics(prefix='test_')\n",
    "            })\n",
    "            \n",
    "    def contiguous(self, x):\n",
    "        return x.squeeze(-1).contiguous().type_as(x)\n",
    "\n",
    "    def create_metrics(self, prefix='train_'):\n",
    "        m = nn.ModuleDict()\n",
    "        metrics = torchmetrics.MetricCollection([torchmetrics.Accuracy(), torchmetrics.F1()])\n",
    "        for k in self.outputs_keys:\n",
    "            m[k] = metrics.clone(prefix+k+'_')\n",
    "        return m\n",
    "\n",
    "    def _forward_bert(self, input_ids, token_type_ids, attention_mask):\n",
    "        outputs = self.bert_ner.bert(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        return outputs.last_hidden_state\n",
    "\n",
    "    def _forward_tags(self, last_hidden_state):\n",
    "        tags_outputs = self.bert_ner.dropout(last_hidden_state)\n",
    "        tags_logits = self.bert_ner.classifier(tags_outputs)\n",
    "        return tags_logits.view(-1, self.hparams.tags_size)\n",
    "\n",
    "    def _forward_intent(self, pooled_outputs):\n",
    "        intent_logits = self.intent_network(pooled_outputs)\n",
    "        return intent_logits\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        # tags\n",
    "        last_hidden_state = self._forward_bert(input_ids, token_type_ids, attention_mask)\n",
    "        tags_logits = self._forward_tags(last_hidden_state)\n",
    "\n",
    "        # intent\n",
    "        pooled_outputs = self.bert_pooler(last_hidden_state)\n",
    "        intent_logits = self._forward_intent(pooled_outputs)\n",
    "\n",
    "        return {\n",
    "            'tags': tags_logits,       # (B*max_len, tags_size)\n",
    "            'intent': intent_logits,   # (B, intent_size)\n",
    "        }\n",
    "\n",
    "    def forward_all(self, batch, prefix='train_'):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch['input_ids'], \n",
    "            token_type_ids=batch['token_type_ids'], \n",
    "            attention_mask=batch['attention_mask'], \n",
    "        )\n",
    "\n",
    "        targets = {\n",
    "            'tags': batch['tags'].view(-1),    # (B*max_len, )\n",
    "            'intent': batch['intent'],         # (B, )\n",
    "        }\n",
    "        loss = self.cal_loss(outputs, targets)\n",
    "        self.log(f'{prefix}loss', loss, \n",
    "            on_step=True, on_epoch=True, sync_dist=self.hparams.multigpu)\n",
    "        # logging\n",
    "        self.cal_metrics(outputs, targets, prefix=prefix)\n",
    "        return loss\n",
    "\n",
    "    def cal_loss(self, outputs, targets):\n",
    "        tags_loss = self.loss(outputs['tags'], targets['tags'])\n",
    "        intent_loss = self.loss(outputs['intent'], targets['intent'])\n",
    "        return tags_loss + intent_loss\n",
    "\n",
    "    def cal_metrics(self, outputs, targets, prefix='train_'):\n",
    "        outputs_metrics = defaultdict()\n",
    "        for k in self.outputs_keys:\n",
    "            for k_sub, v in self.metrics[prefix][k](outputs[k], targets[k]).items():\n",
    "                outputs_metrics[k_sub] = v\n",
    "        self.log_dict(outputs_metrics)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.forward_all(batch, prefix='train_')\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.forward_all(batch, prefix='val_')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):   \n",
    "        loss = self.forward_all(batch, prefix='test_')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        lr_schedulers = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "        \n",
    "        return {'optimizer': optimizer, 'lr_scheduler': lr_schedulers, 'monitor': 'val_loss'}\n",
    "\n",
    "    def predict(self, input_ids, token_type_ids, attention_mask):\n",
    "        outputs = self.forward(input_ids, token_type_ids, attention_mask)\n",
    "        predicts = self._predict_from_outputs(outputs)\n",
    "        return predicts\n",
    "\n",
    "    def _predict_from_outputs(self, outputs):\n",
    "        predicts = {k: outputs[k].argmax(-1) for k in ['tags', 'intent']} \n",
    "        return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "hparams = {\n",
    "    'stage': settings['stage'],\n",
    "    'model_path': settings['model_path'], \n",
    "    'intent_size': len(data_module.intents2id), \n",
    "    'tags_size': len(data_module.tags2id), \n",
    "    'max_len': settings['max_len'],\n",
    "    'lr': settings['lr'],\n",
    "    'multigpu': True if settings['n_gpus'] > 1 else False\n",
    "}\n",
    "\n",
    "model = NLUModel(**hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "\n",
    "log_path = src_path / 'logs'\n",
    "checkpoint_path = src_path / 'checkpoints'\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=str(log_path), name=\"NLU\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=str(checkpoint_path), \n",
    "    save_top_k=2,\n",
    "    monitor='val_loss'\n",
    ")\n",
    "progress_callback = TQDMProgressBar(refresh_rate=20)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, \n",
    "    max_epochs=3, \n",
    "    logger=logger, \n",
    "    callbacks=[checkpoint_callback, progress_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type                       | Params\n",
      "----------------------------------------------------------------\n",
      "0 | bert_ner         | BertForTokenClassification | 108 M \n",
      "1 | bert_pooler      | BertPooler                 | 590 K \n",
      "2 | intent_network   | Linear                     | 3.1 K \n",
      "3 | relation_network | RelationNetwork            | 789 K \n",
      "----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "441.356   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model, datamodule=data_module\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab7f212329a491b497f27876271d03c022f2dd26760015eef69af619991238fd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
